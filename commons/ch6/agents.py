# === Imports ===
import logging
import json
from helpers import query_pinecone, call_llm_robust, create_mcp_message

# === 4.1. Context Librarian Agent (Upgraded) ===
def agent_context_librarian(mcp_message, client, index, embedding_model, namespace_context):
    """Retrieves the appropriate Semantic Blueprint from the Context Library."""
    logging.info("[Librarian] Activated. Analyzing intent...")
    try:
        requested_intent = mcp_message['content'].get('intent_query')
        if not requested_intent:
            raise ValueError("Librarian requires 'intent_query' in the input content.")

        results = query_pinecone(
            query_text=requested_intent,
            namespace=namespace_context,
            top_k=1,
            index=index,
            client=client,
            embedding_model=embedding_model
        )

        if results:
            match = results[0]
            logging.info(f"[Librarian] Found blueprint '{match['id']}' (Score: {match['score']:.2f})")
            blueprint_json = match['metadata']['blueprint_json']
            content = {"blueprint_json": blueprint_json}
        else:
            logging.warning("[Librarian] No specific blueprint found. Returning default.")
            content = {"blueprint_json": json.dumps({"instruction": "Generate the content neutrally."})}

        return create_mcp_message("Librarian", content)
    except Exception as e:
        logging.error(f"[Librarian] An error occurred: {e}")
        raise e

# === 4.2. Researcher Agent (Upgraded) ===
def agent_researcher(mcp_message, client, index, generation_model, embedding_model, namespace_knowledge):
    """Retrieves and synthesizes factual information from the Knowledge Base."""
    logging.info("[Researcher] Activated. Investigating topic...")
    try:
        topic = mcp_message['content'].get('topic_query')
        if not topic:
            raise ValueError("Researcher requires 'topic_query' in the input content.")

        results = query_pinecone(
            query_text=topic,
            namespace=namespace_knowledge,
            top_k=3,
            index=index,
            client=client,
            embedding_model=embedding_model
        )

        if not results:
            logging.warning("[Researcher] No relevant information found.")
            return create_mcp_message("Researcher", {"facts": "No data found on the topic."})

        logging.info(f"[Researcher] Found {len(results)} relevant chunks. Synthesizing...")
        source_texts = [match['metadata']['text'] for match in results]
        system_prompt = """You are an expert research synthesis AI.
Synthesize the provided source texts into a concise, bullet-pointed summary answering the user's topic."""
        user_prompt = f"Topic: {topic}\n\nSources:\n" + "\n\n---\n\n".join(source_texts)

        findings = call_llm_robust(
            system_prompt,
            user_prompt,
            client=client,
            generation_model=generation_model
        )
        return create_mcp_message("Researcher", {"facts": findings})
    except Exception as e:
        logging.error(f"[Researcher] An error occurred: {e}")
        raise e

# === 4.3. Writer Agent (Upgraded) ===
# FILE: commons/ch6/agents.py (Correct UPGRADED agent_writer for Chapter 6)
def agent_writer(mcp_message, client, generation_model):
    """Combines research with a blueprint to generate the final output."""
    logging.info("[Writer] Activated. Applying blueprint to source material...")
    try:
        # --- UPGRADE: Unpack structured inputs with added flexibility ---
        blueprint_data = mcp_message['content'].get('blueprint')
        facts_data = mcp_message['content'].get('facts')
        previous_content = mcp_message['content'].get('previous_content')

        blueprint_json_string = blueprint_data.get('blueprint_json') if isinstance(blueprint_data, dict) else blueprint_data

        # ROBUST LOGIC (for Chapter 6) for handling 'facts' or 'summary'
        facts = None
        if isinstance(facts_data, dict):
            # First, try to get 'facts' (from Researcher)
            facts = facts_data.get('facts')
            # If that fails, try to get 'summary' (from Summarizer)
            if facts is None:
                facts = facts_data.get('summary')
        elif isinstance(facts_data, str):
            facts = facts_data

        if not blueprint_json_string or (not facts and not previous_content):
            raise ValueError("Writer requires a blueprint and either 'facts' or 'previous_content'.")

        if facts:
            source_material = facts
            source_label = "SOURCE FACTS"
        else:
            source_material = previous_content
            source_label = "PREVIOUS CONTENT (For Rewriting)"

        system_prompt = f"""You are an expert content generation AI. Your task is to generate content based on the provided SOURCE MATERIAL...""" # (prompt text is unchanged)
        
        user_prompt = f"""--- SOURCE MATERIAL ({source_label}) ---\n{source_material}\n--- END SOURCE MATERIAL ---\n\nGenerate the content now...""" # (prompt text is unchanged)

        final_output = call_llm_robust(
            system_prompt,
            user_prompt,
            client=client,
            generation_model=generation_model
        )
        return create_mcp_message("Writer", final_output)
        
    except Exception as e:
        logging.error(f"[Writer] An error occurred: {e}")
        raise e

logging.info("âœ… Specialist Agents defined and fully upgraded.")

# FILE: Chapter 6
# === 4.4. Summarizer Agent (New for Context Reduction) ===
def agent_summarizer(mcp_message, client, generation_model):
    """
    Reduces a large text to a concise summary based on an objective.
    Acts as a gatekeeper to manage token counts and costs.
    """
    logging.info("[Summarizer] Activated. Reducing context...")
    try:
        # Unpack the inputs from the MCP message
        text_to_summarize = mcp_message['content'].get('text_to_summarize')
        summary_objective = mcp_message['content'].get('summary_objective')

        if not text_to_summarize or not summary_objective:
            raise ValueError("Summarizer requires 'text_to_summarize' and 'summary_objective' in the input content.")

        # Define the prompts for the LLM
        system_prompt = """You are an expert summarization AI. Your task is to reduce the provided text to its essential points, guided by the user's specific objective. The summary must be concise, accurate, and directly address the stated goal."""
        user_prompt = f"""--- OBJECTIVE ---
{summary_objective}

--- TEXT TO SUMMARIZE ---
{text_to_summarize}
--- END TEXT ---

Generate the summary now."""

        # Call the hardened LLM helper to perform the summarization
        summary = call_llm_robust(
            system_prompt,
            user_prompt,
            client=client,
            generation_model=generation_model
        )

        # Return the summary in the standard MCP format
        return create_mcp_message("Summarizer", {"summary": summary})
    except Exception as e:
        logging.error(f"[Summarizer] An error occurred: {e}")
        raise e
