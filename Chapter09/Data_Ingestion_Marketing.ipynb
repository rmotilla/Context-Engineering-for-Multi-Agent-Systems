{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM5LxCWX3sZ2QYJIxNGL5Xr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#High Fidelity Data Ingestion\n",
        "\n",
        "Copyright 2025-2026, Denis Rothman\n",
        "\n",
        "**Goal:** This notebook transforms our basic data pipeline into a high-fidelity ingestion system, a crucial prerequisite for the verifiable, citation-capable AI we are building in Chapter 9. We will simulate the work of a secure \"Data Management Department\" by taking raw source documents and processing them into a structured, metadata-rich knowledge base.\n",
        "\n",
        "This process involves three key steps:\n",
        "\n",
        "* **Prepare a Curated Dataset:** We will create and load several sample Marketing documents, simulating a secure, pre-vetted data source ready for our engine.\n",
        "\n",
        "* **Enrich Data with Source Metadata:** This is the core upgrade. We will modify the ingestion process to tag every single data chunk with its original document source, a critical step that enables verifiability and citations.\n",
        "\n",
        "* **Verify the Ingestion:** We will conclude by running a test query to inspect the vector database and confirm that our high-fidelity metadata has been successfully stored.\n",
        "\n",
        "**January 2026 Upgrade:**\n",
        "In section *2.Initialize Clients*, we can clear the index of its content or append it:\n",
        "```python\n",
        "clear_index = True # If True, empties the index namespaces. If False, appends to existing index.\n",
        "```"
      ],
      "metadata": {
        "id": "FTUldUPH_1jM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Installation and Setup"
      ],
      "metadata": {
        "id": "-1bEq01K2Nmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Installation and Setup\n",
        "# -------------------------------------------------------------------------\n",
        "# We install specific versions for stability and reproducibility.\n",
        "# We include tiktoken for token-based chunking and tenacity for robust API calls."
      ],
      "metadata": {
        "id": "_MlRuXOwA7Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm==4.67.1 --upgrade\n",
        "!pip install openai==1.104.2\n",
        "!pip install pinecone==7.0.0 tqdm==4.67.1 tenacity==8.3.0"
      ],
      "metadata": {
        "id": "NlXCn7y6CQ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032fb739-7eb1-44b3-f641-7b012c7d8306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting openai==1.104.2\n",
            "  Downloading openai-1.104.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==1.104.2) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.104.2) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.104.2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.104.2) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.104.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.104.2) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.104.2) (0.4.2)\n",
            "Downloading openai-1.104.2-py3-none-any.whl (928 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m928.2/928.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 2.12.0\n",
            "    Uninstalling openai-2.12.0:\n",
            "      Successfully uninstalled openai-2.12.0\n",
            "Successfully installed openai-1.104.2\n",
            "Collecting pinecone==7.0.0\n",
            "  Downloading pinecone-7.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting tenacity==8.3.0\n",
            "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2025.11.12)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone==7.0.0)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone==7.0.0) (1.17.0)\n",
            "Downloading pinecone-7.0.0-py3-none-any.whl (516 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: tenacity, pinecone-plugin-interface, pinecone\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.1.2\n",
            "    Uninstalling tenacity-9.1.2:\n",
            "      Successfully uninstalled tenacity-9.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pinecone-7.0.0 pinecone-plugin-interface-0.0.7 tenacity-8.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for this notebook\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import tiktoken\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "# general imports required in the notebooks of this book\n",
        "import re\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "import copy"
      ],
      "metadata": {
        "id": "ptErFjUn54u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "EMBEDDING_DIM = 1536 # Dimension for text-embedding-3-small\n",
        "GENERATION_MODEL = \"gpt-5\""
      ],
      "metadata": {
        "id": "R9fssMtAwGlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc6a312-8853-46fe-e5b1-b0ce50447ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Standard way to access secrets securely in Google Colab\n",
        "    from google.colab import userdata\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        raise ValueError(\"API Keys not found in Colab secrets.\")\n",
        "    print(\"API Keys loaded successfully.\")\n",
        "except ImportError:\n",
        "    # Fallback for non-Colab environments (e.g., local Jupyter)\n",
        "    PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        print(\"Warning: API Keys not found. Ensure environment variables are set.\")"
      ],
      "metadata": {
        "id": "d6V_5MOsBeRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac3e55d-b6f5-4519-89c3-5ab79e4e2092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Keys loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Initialize Clients\n",
        "\n",
        "\n",
        "We can clear the index of its content or append it:\n",
        "\n",
        "```python\n",
        "clear_index=True\n",
        "```\n",
        "\n",
        "**The new data will be APPENDED.** It will **not** overwrite the old data.\n",
        "\n",
        "### The Reason (The ID Logic)\n",
        "\n",
        "The notebook generates IDs for the vectors using this specific line of code:\n",
        "\n",
        "```python\n",
        "chunk_id = f\"{doc_name}_chunk_{total_vectors_uploaded + j}\"\n",
        "\n",
        "```\n",
        "\n",
        "This ID is made of three parts:\n",
        "\n",
        "1. **`doc_name`**: The filename (e.g., `brand_style_guide.txt`).\n",
        "2. **`chunk`**: A static text string.\n",
        "3. **`total_vectors_uploaded + j`**: A numeric counter that starts at 0 every time you run the cell.\n",
        "\n",
        "### Why it Appends (Safe)\n",
        "\n",
        "If the **\"new data\"** consists of files with **different filenames** than the previous run:\n",
        "\n",
        "* **Run 1 (Old Data):** Processed `OldFile.txt`. ID generated: `OldFile.txt_chunk_0`.\n",
        "* **Run 2 (New Data):** You clear the folder and add `NewFile.txt`. The counter resets to 0. ID generated: `NewFile.txt_chunk_0`.\n",
        "\n",
        "Because the filename (`doc_name`) is part of the ID string, `OldFile.txt_chunk_0` is different from `NewFile.txt_chunk_0`. Pinecone sees them as completely different vectors and keeps both.\n",
        "\n",
        "### The Caution (Potential Duplicates)\n",
        "\n",
        "If there is a re-upload of the **exact same file** (same filename) but in a different batch or order:\n",
        "\n",
        "* The counter (`total_vectors_uploaded`) might be different than the first time.\n",
        "* This would generate *new* IDs for the *same* content, resulting in duplicate data in your database rather than an overwrite.\n",
        "\n",
        "**Summary:** As long as your new data has unique filenames compared to the old data, it will cleanly append to the index.\n",
        "\n"
      ],
      "metadata": {
        "id": "dxctIvv62hOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Initialize Clients\n",
        "clear_index = True # If True, empties the index namespaces. If False, appends to existing index.\n",
        "\n",
        "# --- Initialize Clients (assuming this is already done) ---\n",
        "\n",
        "# --- Initialize Pinecone Client ---\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# --- Define Index and Namespaces (assuming this is already done) ---\n",
        "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
        "NAMESPACE_KNOWLEDGE = \"KnowledgeStore\"\n",
        "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
        "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "# Check if index exists\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    print(f\"Index '{INDEX_NAME}' not found. Creating new serverless index...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=EMBEDDING_DIM,\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # Wait for index to be ready\n",
        "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        time.sleep(1)\n",
        "    print(\"Index created successfully. It is new and empty.\")\n",
        "else:\n",
        "    print(f\"Index '{INDEX_NAME}' already exists.\")\n",
        "\n",
        "    if clear_index:\n",
        "        print(\"clear_index=True. Clearing namespaces for a fresh start...\")\n",
        "        index = pc.Index(INDEX_NAME)\n",
        "        namespaces_to_clear = [NAMESPACE_KNOWLEDGE, NAMESPACE_CONTEXT]\n",
        "\n",
        "        for namespace in namespaces_to_clear:\n",
        "            # Check if namespace exists and has vectors before deleting\n",
        "            stats = index.describe_index_stats()\n",
        "            if namespace in stats.namespaces and stats.namespaces[namespace].vector_count > 0:\n",
        "                print(f\"Clearing namespace '{namespace}'...\")\n",
        "                index.delete(delete_all=True, namespace=namespace)\n",
        "\n",
        "                # **CRITICAL FUNCTTION: Wait for deletion to complete**\n",
        "                while True:\n",
        "                    stats = index.describe_index_stats()\n",
        "                    if namespace not in stats.namespaces or stats.namespaces[namespace].vector_count == 0:\n",
        "                        print(f\"Namespace '{namespace}' cleared successfully.\")\n",
        "                        break\n",
        "                    print(f\"Waiting for namespace '{namespace}' to clear...\")\n",
        "                    time.sleep(5) # Poll every 5 seconds\n",
        "            else:\n",
        "                print(f\"Namespace '{namespace}' is already empty or does not exist. Skipping.\")\n",
        "    else:\n",
        "        print(\"clear_index=False. Index will not be emptied (Append mode).\")\n",
        "\n",
        "# Connect to the index for subsequent operations\n",
        "index = pc.Index(INDEX_NAME)"
      ],
      "metadata": {
        "id": "yqAbeOskEjP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80796d91-228d-4f80-f662-75d4480962b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'genai-mas-mcp-ch3' already exists.\n",
            "clear_index=False. Index will not be emptied (Append mode).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Data Preparation: The Context Library (Procedural RAG)"
      ],
      "metadata": {
        "id": "hXkeOtzx2Ws-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory to store our source documents\n",
        "if not os.path.exists(\"marketing_documents\"):\n",
        "    os.makedirs(\"marketing_documents\")"
      ],
      "metadata": {
        "id": "-pGyOCaXa8AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Document 1: Brand Style Guide\n",
        "brand_style_guide_text = \"\"\"\n",
        "Brand Voice and Tone Guide: \"Innovate Forward\"\n",
        "\n",
        "Our brand voice is guided by three core principles: Clarity, Confidence, and Aspiration. We are expert guides, not academic lecturers. Our tone should always be empowering, forward-looking, and accessible.\n",
        "\n",
        "1. Clarity:\n",
        "- Use simple, direct language. Avoid jargon and overly technical terms.\n",
        "- Prefer short, declarative sentences.\n",
        "- Structure content with clear headings and bullet points for scannability.\n",
        "- Goal: Make complex topics feel simple and understandable.\n",
        "\n",
        "2. Confidence:\n",
        "- Use an active voice. (e.g., \"Our system delivers results,\" not \"Results are delivered by our system.\")\n",
        "- Be authoritative but not arrogant. State facts and benefits directly.\n",
        "- Avoid hedging language like \"might,\" \"could,\" or \"perhaps.\"\n",
        "- Goal: Instill trust and convey expertise.\n",
        "\n",
        "3. Aspiration:\n",
        "- Focus on the benefits, not just the features. Frame our product as a tool for achieving a better future.\n",
        "- Use forward-looking and positive language (e.g., \"imagine,\" \"transform,\" \"unlock\").\n",
        "- Speak to the user's goals and ambitions.\n",
        "- Goal: Inspire the user and connect our brand to their success.\n",
        "\n",
        "Forbidden Language:\n",
        "- Never use overly casual slang or unprofessional language.\n",
        "- Do not make specific, quantitative promises that cannot be universally guaranteed (e.g., \"You will increase profits by 300%\").\n",
        "- Avoid negative comparisons to competitors. Focus on our strengths.\n",
        "\"\"\"\n",
        "with open(\"marketing_documents/brand_style_guide.txt\", \"w\") as f:\n",
        "    f.write(brand_style_guide_text)\n",
        "\n",
        "print(\"‚úÖ Created marketing_documents/brand_style_guide.txt\")"
      ],
      "metadata": {
        "id": "wZLdgZ5bkgnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ac9e08-2f58-445e-924f-570164feddfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created marketing_documents/brand_style_guide.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Document 2: Product Spec Sheet\n",
        "product_spec_sheet_text = \"\"\"\n",
        "Product Specification Sheet: Project QuantumDrive\n",
        "\n",
        "Product Name: QuantumDrive Q-1\n",
        "Product Type: Solid-State Drive (SSD)\n",
        "Target Market: Creative Professionals (Video Editors, 3D Artists, Photographers)\n",
        "\n",
        "Core Features:\n",
        "- Storage Capacity: Available in 2TB, 4TB, and 8TB models.\n",
        "- Read Speed: Sequential read speeds up to 7,500 MB/s.\n",
        "- Write Speed: Sequential write speeds up to 7,000 MB/s.\n",
        "- Interface: NVMe 2.0, PCIe Gen 5.\n",
        "- Endurance Rating: 3,000 Terabytes Written (TBW) for 4TB model.\n",
        "- Cooling System: Integrated graphene heat spreader. Prevents thermal throttling under sustained load.\n",
        "- Software: Includes \"DataWeaver\" backup and encryption suite. AES-256 bit hardware encryption.\n",
        "- Warranty: 5-year limited warranty.\n",
        "\"\"\"\n",
        "with open(\"marketing_documents/product_spec_sheet.txt\", \"w\") as f:\n",
        "    f.write(product_spec_sheet_text)\n",
        "\n",
        "print(\"‚úÖ Created marketing_documents/product_spec_sheet.txt\")"
      ],
      "metadata": {
        "id": "6yG8yX58khcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5d5d63a-233d-44e2-aa76-9ff196d48ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created marketing_documents/product_spec_sheet.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Document 3: Competitor Press Release\n",
        "competitor_press_release_text = \"\"\"\n",
        "FOR IMMEDIATE RELEASE\n",
        "\n",
        "ChronoTech Unveils the Chrono SSD Pro: Speed for the Modern Creator\n",
        "\n",
        "CUPERTINO, CA ‚Äì ChronoTech today announced the launch of the Chrono SSD Pro, its new flagship solid-state drive. Aimed at digital artists and content creators, the Chrono SSD Pro prioritizes raw performance to reduce workflow bottlenecks.\n",
        "\n",
        "\"Creators are tired of waiting. The Chrono SSD Pro is our answer,\" said Jane Doe, CEO of ChronoTech. \"We've focused on delivering the fastest possible read and write speeds to ensure that technology never gets in the way of creativity.\"\n",
        "\n",
        "The new drive boasts sequential read speeds of 7,300 MB/s and is built on the proven PCIe Gen 4 platform. ChronoTech is emphasizing its value proposition, offering the 4TB model at a highly competitive price point. The Chrono SSD Pro is available for purchase today.\n",
        "\"\"\"\n",
        "with open(\"marketing_documents/competitor_press_release.txt\", \"w\") as f:\n",
        "    f.write(competitor_press_release_text)\n",
        "\n",
        "print(\"‚úÖ Created marketing_documents/competitor_press_release.txt\")"
      ],
      "metadata": {
        "id": "Y9ssSsDEklK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f36cbb-103c-458d-ccba-b17038024d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created marketing_documents/competitor_press_release.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Document 4: Social Media Brief\n",
        "social_media_brief_text = \"\"\"\n",
        "Social Media Campaign Brief: QuantumDrive Q1 Launch\n",
        "\n",
        "Campaign Goal: Generate excitement and drive pre-orders for the new QuantumDrive Q-1.\n",
        "\n",
        "Target Audience:\n",
        "- Primary: Professional video editors and 3D artists on LinkedIn and Twitter.\n",
        "- Secondary: Tech enthusiasts and PC builders on Instagram and Reddit.\n",
        "\n",
        "Key Messages:\n",
        "1. End the Wait: Focus on the theme of speed. Emphasize how the QuantumDrive eliminates rendering and loading times.\n",
        "2. Built for Pros: Highlight the professional-grade features like the graphene heat spreader and hardware encryption.\n",
        "3. The Ultimate Upgrade: Position the QuantumDrive as the single most impactful upgrade a creative professional can make to their workstation.\n",
        "\n",
        "Call to Action (CTA): Drive users to the pre-order page on our website. Use a trackable link.\n",
        "\n",
        "Hashtags: #QuantumDrive #EndTheWait #BuiltForPros #SSD\n",
        "\"\"\"\n",
        "with open(\"marketing_documents/social_media_brief.txt\", \"w\") as f:\n",
        "    f.write(social_media_brief_text)\n",
        "\n",
        "print(\"‚úÖ Created marketing_documents/social_media_brief.txt\")"
      ],
      "metadata": {
        "id": "d1853f8AkpY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d18a6d38-79d0-4eef-b3ea-59a994e2d048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created marketing_documents/social_media_brief.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Document 5: SEO Keywords\n",
        "seo_keywords_text = \"\"\"\n",
        "SEO Target Keywords & Topics - 2025\n",
        "\n",
        "Primary Keyword: \"best ssd for video editing\"\n",
        "\n",
        "Secondary Keywords:\n",
        "- \"fastest ssd for 4k video\"\n",
        "- \"nvme gen 5 ssd\"\n",
        "- \"high endurance ssd for professionals\"\n",
        "- \"video editing storage solutions\"\n",
        "\n",
        "Content Goals:\n",
        "- Create a pillar page for \"The Ultimate Guide to Video Editing Storage.\"\n",
        "- Write supporting blog posts for each of the secondary keywords.\n",
        "- Ensure all content is authoritative, helpful, and links back to the QuantumDrive product page where appropriate.\n",
        "- Target a technical but accessible tone.\n",
        "\"\"\"\n",
        "with open(\"marketing_documents/seo_keywords.txt\", \"w\") as f:\n",
        "    f.write(seo_keywords_text)\n",
        "\n",
        "print(\"‚úÖ Created marketing_documents/seo_keywords.txt\")"
      ],
      "metadata": {
        "id": "8oMtczNokuJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1c530d0-ef5f-4660-daf0-ad04da67f5e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created marketing_documents/seo_keywords.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Document 6: Customer Interview Notes\n",
        "customer_interview_notes_text = \"\"\"\n",
        "Customer Interview Notes: Maria R., Freelance Video Editor\n",
        "\n",
        "Background:\n",
        "- Works with 4K and 6K video files from multiple clients.\n",
        "- Current workstation is 2 years old.\n",
        "- Struggles with project deadlines.\n",
        "\n",
        "Pain Points:\n",
        "- \"My current drive is the bottleneck. I spend hours just waiting for files to transfer or for a timeline to render. It's dead time.\"\n",
        "- \"Had a drive fail on me last year. Lost a whole project. Now I'm paranoid about backups, which takes even more time.\"\n",
        "- \"When a drive overheats, the speed drops, and my whole system grinds to a halt right in the middle of a critical render. It's incredibly frustrating.\"\n",
        "\n",
        "Goals:\n",
        "- Wants to reduce wasted time and take on more client work.\n",
        "- Needs a storage solution that is not just fast, but reliable and secure.\n",
        "- \"I just want my tools to disappear. I want to focus on the creative work, not the hardware.\"\n",
        "\"\"\"\n",
        "with open(\"marketing_documents/customer_interview_notes.txt\", \"w\") as f:\n",
        "    f.write(customer_interview_notes_text)\n",
        "\n",
        "print(\"‚úÖ Created marketing_documents/customer_interview_notes.txt\")"
      ],
      "metadata": {
        "id": "bJ2zNTm8kxAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf188af-87c7-4b49-8b2b-7756e9d3126e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created marketing_documents/customer_interview_notes.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Document 7: Email Nurture Outline\n",
        "email_nurture_outline_text = \"\"\"\n",
        "Email Nurture Sequence Outline: New Lead Follow-Up\n",
        "\n",
        "Audience: Users who downloaded our \"Video Editing Storage Guide.\"\n",
        "Goal: Nurture the lead and guide them toward a purchase of the QuantumDrive.\n",
        "\n",
        "Email 1: The Problem (Send 1 day after download)\n",
        "- Objective: Acknowledge their pain point (slow storage).\n",
        "- Content: Briefly introduce the concept of workflow bottlenecks and how they kill creativity.\n",
        "- CTA: \"Is slow storage holding you back?\" (No product mention yet).\n",
        "\n",
        "Email 2: The Solution (Send 3 days after download)\n",
        "- Objective: Introduce the QuantumDrive as the solution.\n",
        "- Content: Highlight the key benefits from the spec sheet (speed, reliability). Focus on the \"End the Wait\" message.\n",
        "- CTA: Link to the QuantumDrive product page.\n",
        "\n",
        "Email 3: The Proof (Send 5 days after download)\n",
        "- Objective: Build trust with social proof.\n",
        "- Content: (Fictional) Include a short testimonial from a professional editor. Reiterate the 5-year warranty.\n",
        "- CTA: \"Ready to upgrade? Pre-order your QuantumDrive today.\"\n",
        "\"\"\"\n",
        "with open(\"marketing_documents/email_nurture_outline.txt\", \"w\") as f:\n",
        "    f.write(email_nurture_outline_text)\n",
        "\n",
        "print(\"‚úÖ Created marketing_documents/email_nurture_outline.txt\")"
      ],
      "metadata": {
        "id": "VL88WmD7k9c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e823e5-7ff7-40f6-a1d6-25fa3c46949e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created marketing_documents/email_nurture_outline.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.Data Preparation: The Context Library (Procedural RAG)\n",
        "# -------------------------------------------------------------------------\n",
        "# We define the Semantic Blueprints derived from Chapter 1.\n",
        "# CRITICAL: We embed the 'description' (the intent), so the Librarian agent\n",
        "# can find the right blueprint based on the desired style. The 'blueprint'\n",
        "# itself is stored as metadata.\n",
        "\n",
        "context_blueprints = [\n",
        "    {\n",
        "        \"id\": \"blueprint_suspense_narrative\",\n",
        "        \"description\": \"A precise Semantic Blueprint designed to generate suspenseful and tense narratives, suitable for children's stories. Focuses on atmosphere, perceived threats, and emotional impact. Ideal for creative writing.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Increase tension and create suspense.\",\n",
        "              \"style_guide\": \"Use short, sharp sentences. Focus on sensory details (sounds, shadows). Maintain a slightly eerie but age-appropriate tone.\",\n",
        "              \"participants\": [\n",
        "                { \"role\": \"Agent\", \"description\": \"The protagonist experiencing the events.\" },\n",
        "                { \"role\": \"Source_of_Threat\", \"description\": \"The underlying danger or mystery.\" }\n",
        "              ],\n",
        "            \"instruction\": \"Rewrite the provided facts into a narrative adhering strictly to the scene_goal and style_guide.\"\n",
        "            })\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"blueprint_technical_explanation\",\n",
        "        \"description\": \"A Semantic Blueprint designed for technical explanation or analysis. This blueprint focuses on clarity, objectivity, and structure. Ideal for breaking down complex processes, explaining mechanisms, or summarizing scientific findings.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Explain the mechanism or findings clearly and concisely.\",\n",
        "              \"style_guide\": \"Maintain an objective and formal tone. Use precise terminology. Prioritize factual accuracy and clarity over narrative flair.\",\n",
        "              \"structure\": [\"Definition\", \"Function/Operation\", \"Key Findings/Impact\"],\n",
        "              \"instruction\": \"Organize the provided facts into the defined structure, adhering to the style_guide.\"\n",
        "            })\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"blueprint_casual_summary\",\n",
        "        \"description\": \"A goal-oriented context for creating a casual, easy-to-read summary. Focuses on brevity and accessibility, explaining concepts simply.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Summarize information quickly and casually.\",\n",
        "              \"style_guide\": \"Use informal language. Keep it brief and engaging. Imagine explaining it to a friend.\",\n",
        "              \"instruction\": \"Summarize the provided facts using the casual style guide.\"\n",
        "            })\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"\\nPrepared {len(context_blueprints)} context blueprints.\")"
      ],
      "metadata": {
        "id": "-XaDNRzlGP1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be19373f-2eab-4124-b90d-cc693ff85d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prepared 3 context blueprints.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Updating the Data Loading and Processing Logic\n",
        "# -------------------------------------------------------------------------\n",
        "# Load all documents from our new directory\n",
        "knowledge_base = {}\n",
        "doc_dir = \"marketing_documents\"\n",
        "for filename in os.listdir(doc_dir):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        with open(os.path.join(doc_dir, filename), 'r') as f:\n",
        "            knowledge_base[filename] = f.read()\n",
        "\n",
        "print(f\"üìö Loaded {len(knowledge_base)} documents into the knowledge base.\")"
      ],
      "metadata": {
        "id": "GitdjWeYGrum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86af9cad-40c5-4e81-b0b5-214e4c86ce1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Loaded 7 documents into the knowledge base.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.Helper Functions for Chunking and Embedding\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Initialize tokenizer for robust, token-aware chunking\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def chunk_text(text, chunk_size=400, overlap=50):\n",
        "    \"\"\"Chunks text based on token count with overlap (Best practice for RAG).\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), chunk_size - overlap):\n",
        "        chunk_tokens = tokens[i:i + chunk_size]\n",
        "        chunk_text = tokenizer.decode(chunk_tokens)\n",
        "        # Basic cleanup\n",
        "        chunk_text = chunk_text.replace(\"\\n\", \" \").strip()\n",
        "        if chunk_text:\n",
        "            chunks.append(chunk_text)\n",
        "    return chunks\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embeddings_batch(texts, model=EMBEDDING_MODEL):\n",
        "    \"\"\"Generates embeddings for a batch of texts using OpenAI, with retries.\"\"\"\n",
        "    # OpenAI expects the input texts to have newlines replaced by spaces\n",
        "    texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
        "    response = client.embeddings.create(input=texts, model=model)\n",
        "    return [item.embedding for item in response.data]\n"
      ],
      "metadata": {
        "id": "5qqwP4AfG0ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nYQq4M8_vP8"
      },
      "outputs": [],
      "source": [
        "#@title Process and Upload Data (High-Fidelity Version)\n",
        "\n",
        "# --- 6.1. Context Library (No Changes) ---\n",
        "print(f\"\\nProcessing and uploading Context Library to namespace: {NAMESPACE_CONTEXT}\")\n",
        "# ... (The existing code for context_blueprints remains the same) ...\n",
        "vectors_context = []\n",
        "for item in tqdm(context_blueprints):\n",
        "    embedding = get_embeddings_batch([item['description']])[0]\n",
        "    vectors_context.append({\n",
        "        \"id\": item['id'],\n",
        "        \"values\": embedding,\n",
        "        \"metadata\": { \"description\": item['description'], \"blueprint_json\": item['blueprint'] }\n",
        "    })\n",
        "if vectors_context:\n",
        "    index.upsert(vectors=vectors_context, namespace=NAMESPACE_CONTEXT)\n",
        "    print(f\"Successfully uploaded {len(vectors_context)} context vectors.\")\n",
        "\n",
        "# --- 6.2. Knowledge Base (UPGRADED FOR HIGH-FIDELITY RAG) ---\n",
        "print(f\"\\nProcessing and uploading Knowledge Base to namespace: {NAMESPACE_KNOWLEDGE}\")\n",
        "batch_size = 100\n",
        "total_vectors_uploaded = 0\n",
        "\n",
        "for doc_name, doc_content in knowledge_base.items():\n",
        "    print(f\"  - Processing document: {doc_name}\")\n",
        "    # Chunk the document content\n",
        "    knowledge_chunks = chunk_text(doc_content)\n",
        "\n",
        "    # Process in batches\n",
        "    for i in tqdm(range(0, len(knowledge_chunks), batch_size), desc=f\"  Uploading {doc_name}\"):\n",
        "        batch_texts = knowledge_chunks[i:i+batch_size]\n",
        "        batch_embeddings = get_embeddings_batch(batch_texts)\n",
        "\n",
        "        batch_vectors = []\n",
        "        for j, embedding in enumerate(batch_embeddings):\n",
        "            chunk_id = f\"{doc_name}_chunk_{total_vectors_uploaded + j}\"\n",
        "\n",
        "            # CRITICAL UPGRADE: Add the 'source' document name to the metadata\n",
        "            batch_vectors.append({\n",
        "                \"id\": chunk_id,\n",
        "                \"values\": embedding,\n",
        "                \"metadata\": {\n",
        "                    \"text\": batch_texts[j],\n",
        "                    \"source\": doc_name  # This is the key to verifiability\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # Upsert the batch\n",
        "        index.upsert(vectors=batch_vectors, namespace=NAMESPACE_KNOWLEDGE)\n",
        "\n",
        "    total_vectors_uploaded += len(knowledge_chunks)\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully uploaded {total_vectors_uploaded} knowledge vectors from {len(knowledge_base)} documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.Final Verification\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\nIngestion complete. Final Pinecone Index Stats (may take a moment to update):\")\n",
        "time.sleep(15) # Give Pinecone a moment to update stats\n",
        "print(index.describe_index_stats())"
      ],
      "metadata": {
        "id": "6uF27i5EHLjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d73fbaa7-2463-4d9c-d9e2-ba9f34411711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ingestion complete. Final Pinecone Index Stats (may take a moment to update):\n",
            "{'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {'ContextLibrary': {'vector_count': 3},\n",
            "                'KnowledgeStore': {'vector_count': 10}},\n",
            " 'total_vector_count': 13,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Verify Metadata Ingestion\n",
        "# This step confirms our 'source' metadata was successfully added.\n",
        "import pprint\n",
        "print(\"Querying a sample vector to verify metadata...\")\n",
        "\n",
        "# Get embedding for a sample query\n",
        "query_embedding = get_embeddings_batch([\"Sum up the lead follow up\"])[0]\n",
        "\n",
        "# Query Pinecone\n",
        "results = index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=1,\n",
        "    namespace=NAMESPACE_KNOWLEDGE,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "# Print the metadata of the top result\n",
        "if results['matches']:\n",
        "    top_match_metadata = results['matches'][0]['metadata']\n",
        "    print(\"\\n‚úÖ Verification successful! Metadata of top match:\")\n",
        "    pprint.pprint(top_match_metadata)\n",
        "else:\n",
        "    print(\"‚ùå Verification failed. No results found.\")"
      ],
      "metadata": {
        "id": "ZKa3oRN-fEk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e934c588-9d96-4d37-82d9-c63fc978dce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying a sample vector to verify metadata...\n",
            "\n",
            "‚úÖ Verification successful! Metadata of top match:\n",
            "{'source': 'email_nurture_outline.txt',\n",
            " 'text': 'Email Nurture Sequence Outline: New Lead Follow-Up  Audience: Users '\n",
            "         'who downloaded our \"Video Editing Storage Guide.\" Goal: Nurture the '\n",
            "         'lead and guide them toward a purchase of the QuantumDrive.  Email 1: '\n",
            "         'The Problem (Send 1 day after download) - Objective: Acknowledge '\n",
            "         'their pain point (slow storage). - Content: Briefly introduce the '\n",
            "         'concept of workflow bottlenecks and how they kill creativity. - CTA: '\n",
            "         '\"Is slow storage holding you back?\" (No product mention yet).  Email '\n",
            "         '2: The Solution (Send 3 days after download) - Objective: Introduce '\n",
            "         'the QuantumDrive as the solution. - Content: Highlight the key '\n",
            "         'benefits from the spec sheet (speed, reliability). Focus on the \"End '\n",
            "         'the Wait\" message. - CTA: Link to the QuantumDrive product page.  '\n",
            "         'Email 3: The Proof (Send 5 days after download) - Objective: Build '\n",
            "         'trust with social proof. - Content: (Fictional) Include a short '\n",
            "         'testimonial from a professional editor. Reiterate the 5-year '\n",
            "         'warranty. - CTA: \"Ready to upgrade? Pre-order your QuantumDrive '\n",
            "         'today.\"'}\n"
          ]
        }
      ]
    }
  ]
}