{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Meeting Analysis Use Case\n",
        "\n",
        "Copyright 2025, Denis Rothman\n",
        "\n",
        "The notebook's goal is to demonstrate how to guide an AI through a multi-step analytical process, moving from a raw transcript to actionable insights, thereby training both the user and the AI.\n",
        "\n",
        "This notebook is a step-by-step guide on how you can engineer a structured context . This guide will serve as the architectural blueprint for your code, explaining not just the *what* but the *why* at each stage.\n"
      ],
      "metadata": {
        "id": "FDBDBUKfSggD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Setup and Preliminaries\n",
        "\n",
        "This section handles the basic configuration.\n",
        "\n",
        "1.  **Cell 1: Install Libraries**\n",
        "\n",
        "      * Install the necessary OpenAI library."
      ],
      "metadata": {
        "id": "ivJQbp_mwtDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Installation\n",
        "!pip install openai"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "voHBQKGtSggF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 2: Imports and API Key**\n",
        "\n",
        "      * Import the library and securely prompt for the user's API key. This is better than hard-coding it."
      ],
      "metadata": {
        "id": "jtGvDLpLSggG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "eNHm5qkhSggG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a121f7-5871-4868-81b6-3173df7a2f34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  **Cell 3: The Raw Data (The \"Crime Scene\")**\n",
        "\n",
        "      * Define the meeting transcript as a multi-line string. This is our primary data source."
      ],
      "metadata": {
        "id": "Vgp62z7OSggG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: The Full Meeting Transcript\n",
        "meeting_transcript = \"\"\"\n",
        "        Tom: Morning all. Coffee is still kicking in.\n",
        "        Sarah: Morning, Tom. Right, let's jump in. Project Phoenix timeline. Tom, you said the backend components are on track?\n",
        "        Tom: Mostly. We hit a small snag with the payment gateway integration. It's... more complex than the docs suggested. We might need another three days.\n",
        "        Maria: Three days? Tom, that's going to push the final testing phase right up against the launch deadline. We don't have that buffer.\n",
        "        Sarah: I agree with Maria. What's the alternative, Tom?\n",
        "        Tom: I suppose I could work over the weekend to catch up. I'd rather not, but I can see the bind we're in.\n",
        "        Sarah: Appreciate that, Tom. Let's tentatively agree on that. Maria, what about the front-end?\n",
        "        Maria: We're good. In fact, we're a bit ahead. We have some extra bandwidth.\n",
        "        Sarah: Excellent. Okay, one last thing. The marketing team wants to do a big social media push on launch day. Thoughts?\n",
        "        Tom: Seems standard.\n",
        "        Maria: I think that's a mistake. A big push on day one will swamp our servers if there are any initial bugs. We should do a soft launch, invite-only for the first week, and then do the big push. More controlled.\n",
        "        Sarah: That's a very good point, Maria. A much safer strategy. Let's go with that. Okay, great meeting. I'll send out a summary.\n",
        "        Tom: Sounds good. Now, more coffee.\n",
        "        \"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UJpZUVXpSggG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 1: Establishing the scope (The 'What')\n",
        "\n",
        "Here, we define the scope of the analysis. Each step's output will inform the next, creating a chain of context.\n",
        "\n",
        "1.  **Cell 4: g2 - Set the Signal-to-Noise Ratio**\n",
        "\n",
        "      * We'll start by cleaning the data. The prompt explicitly tells the AI to separate substantive content from conversational noise."
      ],
      "metadata": {
        "id": "40PTjmLaSggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: g2 - Isolating Content from Noise\n",
        "prompt_g2 = f\"\"\"\n",
        "        Analyze the following meeting transcript. Your task is to isolate the substantive content from the conversational noise.\n",
        "        - Substantive content includes: decisions made, project updates, problems raised, and strategic suggestions.\n",
        "        - Noise includes: greetings, pleasantries, and off-topic remarks (like coffee).\n",
        "        Return ONLY the substantive content.\n",
        "\n",
        "        Transcript:\n",
        "        ---\n",
        "        {meeting_transcript}\n",
        "        ---\n",
        "        \"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "069eAXt9SggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "try:\n",
        "    client = OpenAI()\n",
        "\n",
        "    response_g2 = client.chat.completions.create(\n",
        "        model=\"gpt-5\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt_g2}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    substantive_content = response_g2.choices[0].message.content\n",
        "    print(\"--- SUBSTANTIVE CONTENT ---\")\n",
        "    print(substantive_content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptfVZHlps3r4",
        "outputId": "c6a5ed47-aeb5-491b-d417-a7ceb61f868e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SUBSTANTIVE CONTENT ---\n",
            "- Project Phoenix timeline: Backend mostly on track, but payment gateway integration is more complex than expected; needs an additional three days.\n",
            "- Impact: Extra three days would push final testing up against the launch deadline, reducing buffer.\n",
            "- Mitigation decision: Tom will work over the weekend to catch up (tentatively agreed).\n",
            "- Front-end status: Ahead of schedule with extra bandwidth.\n",
            "- Marketing/launch strategy: Initial plan for a big social media push on launch day flagged as risky (potential server load with early bugs). Decision: Use a soft launch (invite-only) for the first week, then execute the big push.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 5: g3 - Define the Scope of Time (Simulated RAG)**\n",
        "\n",
        "      * We'll simulate a RAG context by providing a \"previous\" summary and asking for what's new. This teaches the user the importance of historical context."
      ],
      "metadata": {
        "id": "XY2ZlN8sSggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: g3 - Identifying NEW Information (Simulated RAG)\n",
        "previous_summary = \"In our last meeting, we finalized the goals for Project Phoenix and assigned backend work to Tom and front-end to Maria.\"\n",
        "\n",
        "prompt_g3 = f\"\"\"\n",
        "Context: The summary of our last meeting was: \"{previous_summary}\"\n",
        "\n",
        "Task: Analyze the following substantive content from our new meeting. Identify and summarize ONLY the new developments, problems, or decisions that have occurred since the last meeting.\n",
        "\n",
        "New Meeting Content:\n",
        "---\n",
        "{substantive_content}\n",
        "---\n",
        "\"\"\"\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Your chat completion request\n",
        "try:\n",
        "    response_g3 = client.chat.completions.create(\n",
        "        model=\"gpt-5\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g3}]\n",
        "    )\n",
        "    new_developments = response_g3.choices[0].message.content\n",
        "    print(\"--- NEW DEVELOPMENTS SINCE LAST MEETING ---\")\n",
        "    print(new_developments)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NEW DEVELOPMENTS SINCE LAST MEETING ---\n",
            "- Backend issue: Payment gateway integration is more complex than expected; needs an additional three days.\n",
            "- Schedule impact: The extra three days compress final testing, pushing it up against the launch deadline and reducing buffer.\n",
            "- Mitigation decision: Tentative agreement that Tom will work over the weekend to catch up.\n",
            "- Front-end status: Ahead of schedule with extra bandwidth.\n",
            "- Launch/marketing decision: Shift from a big day-one social push to a one-week invite-only soft launch, followed by the major push.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "1LhMAk4-SggH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4acc08b-92ea-4750-c081-13032f139444"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 2: Conducting the Investigation (The 'How')\n",
        "\n",
        "Now we move from identifying facts to generating insights, the core of the semantic context interpreation journey.\n",
        "\n",
        "1.  **Cell 6: g4 - Identify the Key Threads**\n",
        "\n",
        "      * This is a crucial step. The prompt asks the AI to read between the lines."
      ],
      "metadata": {
        "id": "wh2UWr4ySggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: g4 - Uncovering Implicit Threads\n",
        "prompt_g4 = f\"\"\"\n",
        "Task: Analyze the following meeting content for implicit social dynamics and unstated feelings. Go beyond the literal words.\n",
        "- Did anyone seem hesitant or reluctant despite agreeing to something?\n",
        "- Were there any underlying disagreements or tensions?\n",
        "- What was the overall mood?\n",
        "\n",
        "Meeting Content:\n",
        "---\n",
        "{substantive_content}\n",
        "---\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g4 = client.chat.completions.create(\n",
        "        model=\"gpt-5\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g4}]\n",
        "    )\n",
        "    implicit_threads = response_g4.choices[0].message.content\n",
        "    print(\"--- IMPLICIT THREADS AND DYNAMICS ---\")\n",
        "    print(implicit_threads)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- IMPLICIT THREADS AND DYNAMICS ---\n",
            "Here’s what seems to be happening beneath the surface:\n",
            "\n",
            "Hesitation/reluctance despite agreement\n",
            "- Tom’s “tentative” agreement to work over the weekend reads as reluctant. It suggests he felt pressure to volunteer rather than genuine willingness.\n",
            "- Marketing likely agreed to the soft launch with some reluctance; shifting from a big day-one push to invite-only implies a concession to engineering risk.\n",
            "\n",
            "Underlying disagreements or tensions\n",
            "- Pace vs quality: Engineering wants stability and buffer; marketing originally aimed for impact. The soft launch is a compromise, but the differing risk appetites remain.\n",
            "- Workload equity: Backend is behind while frontend has “extra bandwidth.” The decision to have Tom work the weekend (vs redistributing tasks) hints at siloing or a norm of individual heroics, which can breed quiet resentment.\n",
            "- Testing squeeze: Pushing testing against the deadline implies QA will be under pressure, potentially creating friction if bugs slip through or late changes occur.\n",
            "- Estimation confidence: The payment gateway being “more complex than expected” may subtly challenge earlier estimates, inviting unspoken doubt about planning or vendor integration assumptions.\n",
            "\n",
            "Overall mood\n",
            "- Sober, pragmatic, and slightly tense. The group is solution-oriented and collaborative, but there’s a sense of urgency and strain, with relief at having a plan tempered by concerns about workload, risk, and reduced buffer.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "Tkb3x_cnSggI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e35387-7e54-4018-84f2-60f9fd60b97a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 7: g5 - Perform \"Intellectual Combinations\"**\n",
        "\n",
        "      * Here, we prompt the AI to be creative and solve a problem by synthesizing different ideas from the meeting."
      ],
      "metadata": {
        "id": "yyd_EZ5HSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: g5 - Generating a Novel Solution\n",
        "prompt_g5 = f\"\"\"\n",
        "Context: In the meeting, Maria suggested a 'soft launch' to avoid server strain, and also mentioned her team has 'extra bandwidth'.\n",
        "Tom is facing a 3-day delay on the backend.\n",
        "\n",
        "Task: Propose a novel, actionable idea that uses Maria's team's extra bandwidth to help mitigate Tom's 3-day delay. Combine these two separate pieces of information into a single solution.\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g5 = client.chat.completions.create(\n",
        "        model=\"gpt-5\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g5}]\n",
        "    )\n",
        "    novel_solution = response_g5.choices[0].message.content\n",
        "    print(\"--- NOVEL SOLUTION PROPOSED BY AI ---\")\n",
        "    print(novel_solution)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NOVEL SOLUTION PROPOSED BY AI ---\n",
            "Idea: Soft launch behind a temporary “Edge Bridge” that Maria’s team builds to buffer reads/writes until Tom’s backend is ready.\n",
            "\n",
            "What Maria’s team does (uses their extra bandwidth)\n",
            "- Stand up a thin serverless/API facade that matches the real API contracts (e.g., API Gateway/Lambda + SQS/DynamoDB or Cloudflare Workers + Durable Objects).\n",
            "- Reads: Serve from a prewarmed cache or static snapshots (stale-while-revalidate). Update snapshots hourly via a lightweight data export from staging or existing endpoints.\n",
            "- Writes: Capture requests into a durable queue with idempotency keys; return immediate “queued” success to the client and show “syncing” UI. When Tom’s backend is live, a replay worker drains the queue and applies changes.\n",
            "- Add feature flags/traffic gating (e.g., LaunchDarkly) to limit the soft launch cohort and throttle requests to avoid server strain.\n",
            "\n",
            "How this mitigates the 3-day delay\n",
            "- The product can soft-launch to a small cohort without waiting for the backend; users get read access and buffered writes.\n",
            "- When Tom’s backend is ready, flip routing to the real backend and drain the queue to reconcile data.\n",
            "\n",
            "Action plan and timeline\n",
            "- Day 0 (today): Identify minimal critical endpoints for the soft launch. Classify by read vs write. Define API contracts and idempotency rules. Set success/error thresholds and a kill switch.\n",
            "- Day 1: Maria’s team builds the Edge Bridge, cache, and write queue; implement basic observability and encryption-at-rest for any PII in the queue. Front-end adds “syncing” UI states and feature flags.\n",
            "- Day 2: QA with mocked data, then with a tiny internal cohort. Prewarm caches. Set traffic cap (e.g., 5–10% of target users).\n",
            "- Day 3: Soft launch goes live on the Edge Bridge. When Tom’s backend unlocks, switch routing gradually, start replay worker, monitor for conflicts, then retire the bridge.\n",
            "\n",
            "Risk controls\n",
            "- Data consistency: Use idempotency keys and a simple conflict policy (latest-write-wins or version checks).\n",
            "- Rollback: Feature flag to disable writes or pause replay if error rate exceeds threshold.\n",
            "- Privacy: Encrypt queued payloads; limit PII scope.\n",
            "\n",
            "Owners\n",
            "- Maria’s team: Edge Bridge, caching, queue/replay, monitoring.\n",
            "- Tom’s team: Final backend endpoints, schema, and replay acceptance hooks.\n",
            "- Front-end: Feature-flag routing and “queued/syncing” UX.\n",
            "\n",
            "This combines Maria’s extra bandwidth with a controlled soft launch to keep momentum while absorbing Tom’s 3-day backend delay.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "8JN5E5X4SggI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef5ae20d-aa69-4a79-fbaf-e077a736ff95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 3: Determining the Action (The 'What Next')\n",
        "\n",
        "Finally, we turn the analysis into concrete, forward-looking artifacts.\n",
        "\n",
        "1.  **Cell 8: g6 - Define the Output Format (Final Summary)**\n",
        "\n",
        "      * We compile all the key information into a structured, final summary, showing the importance of clear outputs."
      ],
      "metadata": {
        "id": "J0CD-ztrSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: g6 - Creating the Final, Structured Summary\n",
        "prompt_g6 = f\"\"\"\n",
        "Task: Create a final, concise summary of the meeting in a markdown table.\n",
        "Use the following information to construct the table.\n",
        "\n",
        "- New Developments: {new_developments}\n",
        "\n",
        "The table should have three columns: \"Topic\", \"Decision/Outcome\", and \"Owner\".\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g6 = client.chat.completions.create(\n",
        "        model=\"gpt-5\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g6}]\n",
        "    )\n",
        "    final_summary_table = response_g6.choices[0].message.content\n",
        "    print(\"--- FINAL MEETING SUMMARY TABLE ---\")\n",
        "    print(final_summary_table)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FINAL MEETING SUMMARY TABLE ---\n",
            "| Topic | Decision/Outcome | Owner |\n",
            "|---|---|---|\n",
            "| Backend payment gateway integration | More complex than expected; requires an additional three days | Backend Team |\n",
            "| Schedule impact | Extra three days compress final testing, reducing buffer before launch | Project Manager |\n",
            "| Mitigation | Tentative plan: Tom will work over the weekend to catch up | Tom |\n",
            "| Front-end status | Ahead of schedule with extra bandwidth available | Front-end Team |\n",
            "| Launch/marketing plan | Shift to a one-week invite-only soft launch, then major day-one push | Marketing + Product |\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "xXRnPWuESggI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa21186-5997-4c8e-97c8-80a4ef774a87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 9: g7 - Generate the Subsequent Task**\n",
        "\n",
        "      * The last step is to use the analysis to perform a real-world action, closing the loop from insight to action."
      ],
      "metadata": {
        "id": "jk1g9wVvSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: g7 - Drafting the Follow-Up Action\n",
        "prompt_g7 = f\"\"\"\n",
        "Task: Based on the following summary table, draft a polite and professional follow-up email to the team (Sarah, Tom, Maria).\n",
        "The email should clearly state the decisions made and the action items for each person.\n",
        "\n",
        "Summary Table:\n",
        "---\n",
        "{final_summary_table}\n",
        "---\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g7 = client.chat.completions.create(\n",
        "        model=\"gpt-5\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g7}]\n",
        "    )\n",
        "    follow_up_email = response_g7.choices[0].message.content\n",
        "    print(\"--- DRAFT FOLLOW-UP EMAIL ---\")\n",
        "    print(follow_up_email)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DRAFT FOLLOW-UP EMAIL ---\n",
            "Subject: Follow-up: Decisions and next steps from today’s sync\n",
            "\n",
            "Hi Sarah, Tom, and Maria,\n",
            "\n",
            "Thanks for the productive discussion earlier. Here’s a quick recap of decisions and the action items for each of us.\n",
            "\n",
            "Decisions\n",
            "- Backend payment gateway integration is more complex than expected and will require an additional three days.\n",
            "- This pushes the schedule by three days and compresses the final testing window, reducing our pre-launch buffer.\n",
            "- Mitigation: Tom will work over the weekend to help us catch up.\n",
            "- Front-end is ahead of schedule and has extra bandwidth to support.\n",
            "- Launch/marketing plan will shift to a one-week invite-only soft launch, followed by the larger day-one push.\n",
            "\n",
            "Action items\n",
            "- Tom:\n",
            "  - Confirm weekend availability and share a brief plan (key milestones, dependencies, and any risks).\n",
            "  - Proceed with the gateway integration and coordinate early integration testing with Front-end and QA.\n",
            "  - Provide short daily progress updates and flag blockers immediately.\n",
            "\n",
            "- Sarah:\n",
            "  - Update the project timeline to reflect the three-day shift and the compressed QA window.\n",
            "  - Coordinate with QA on a risk-based test plan that fits the shortened testing period.\n",
            "  - Align with Marketing/Product on the invite-only soft launch scope, success metrics, and comms; circulate the plan to the team.\n",
            "\n",
            "- Maria:\n",
            "  - Reallocate Front-end bandwidth to support the backend integration (payment UI hooks, error handling, instrumentation).\n",
            "  - Partner with Tom on mocks/stubs as needed to unblock early integration and QA.\n",
            "  - Ensure front-end readiness for the soft launch (feature flags/toggles, tracking) and share any gaps.\n",
            "\n",
            "Please reply to confirm your action items and note any constraints or support you need. I’m happy to set up a brief daily check-in while we work through this; propose a time if you have a preference.\n",
            "\n",
            "Thanks all, and appreciate the quick coordination.\n",
            "\n",
            "Best,\n",
            "[Your Name]\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "aQ1PzmTlSggI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac281ec-c240-4b1f-ddd7-4523c9815b23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook structure doesn't just \"get a summary.\" It takes the user on a journey, showing them *how* to think with the AI as a partner. It perfectly translates the abstract \"Scope, Investigation, Action\" framework into a repeatable, educational, and powerful process."
      ],
      "metadata": {
        "id": "-tDIxvHSSggJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}