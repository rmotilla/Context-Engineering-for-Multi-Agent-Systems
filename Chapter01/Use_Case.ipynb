{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Meeting Analysis Use Case\n",
        "\n",
        "Copyright 2025, Denis Rothman\n",
        "\n",
        "The notebook's goal is to demonstrate how to guide an AI through a multi-step analytical process, moving from a raw transcript to actionable insights, thereby training both the user and the AI.\n",
        "\n",
        "This notebook is a step-by-step guide on how you can engineer a structured context . This guide will serve as the architectural blueprint for your code, explaining not just the *what* but the *why* at each stage.\n"
      ],
      "metadata": {
        "id": "FDBDBUKfSggD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Setup and Preliminaries\n",
        "\n",
        "This section handles the basic configuration.\n",
        "\n",
        "1.  **Cell 1: Install Libraries**\n",
        "\n",
        "      * Install the necessary OpenAI library."
      ],
      "metadata": {
        "id": "ivJQbp_mwtDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Installation\n",
        "!pip install openai==2.8.1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.104.2\n",
            "  Downloading openai-1.104.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==1.104.2) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.104.2) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.104.2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.104.2) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.104.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.104.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.104.2) (0.4.2)\n",
            "Downloading openai-1.104.2-py3-none-any.whl (928 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m928.2/928.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.109.1\n",
            "    Uninstalling openai-1.109.1:\n",
            "      Successfully uninstalled openai-1.109.1\n",
            "Successfully installed openai-1.104.2\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "voHBQKGtSggF",
        "outputId": "6fc9c892-e721-4e3f-f39c-d94324d79e75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 2: Imports and API Key**\n",
        "\n",
        "      * Import the library and securely prompt for the user's API key. This is better than hard-coding it."
      ],
      "metadata": {
        "id": "jtGvDLpLSggG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "id": "eNHm5qkhSggG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de09053-4de1-4064-ae30-073edfa69f01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  **Cell 3: The Raw Data (The \"Crime Scene\")**\n",
        "\n",
        "      * Define the meeting transcript as a multi-line string. This is our primary data source."
      ],
      "metadata": {
        "id": "Vgp62z7OSggG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: The Full Meeting Transcript\n",
        "meeting_transcript = \"\"\"\n",
        "        Tom: Morning all. Coffee is still kicking in.\n",
        "        Sarah: Morning, Tom. Right, let's jump in. Project Phoenix timeline. Tom, you said the backend components are on track?\n",
        "        Tom: Mostly. We hit a small snag with the payment gateway integration. It's... more complex than the docs suggested. We might need another three days.\n",
        "        Maria: Three days? Tom, that's going to push the final testing phase right up against the launch deadline. We don't have that buffer.\n",
        "        Sarah: I agree with Maria. What's the alternative, Tom?\n",
        "        Tom: I suppose I could work over the weekend to catch up. I'd rather not, but I can see the bind we're in.\n",
        "        Sarah: Appreciate that, Tom. Let's tentatively agree on that. Maria, what about the front-end?\n",
        "        Maria: We're good. In fact, we're a bit ahead. We have some extra bandwidth.\n",
        "        Sarah: Excellent. Okay, one last thing. The marketing team wants to do a big social media push on launch day. Thoughts?\n",
        "        Tom: Seems standard.\n",
        "        Maria: I think that's a mistake. A big push on day one will swamp our servers if there are any initial bugs. We should do a soft launch, invite-only for the first week, and then do the big push. More controlled.\n",
        "        Sarah: That's a very good point, Maria. A much safer strategy. Let's go with that. Okay, great meeting. I'll send out a summary.\n",
        "        Tom: Sounds good. Now, more coffee.\n",
        "        \"\"\""
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "UJpZUVXpSggG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 1: Establishing the scope (The 'What')\n",
        "\n",
        "Here, we define the scope of the analysis. Each step's output will inform the next, creating a chain of context.\n",
        "\n",
        "1.  **Cell 4: g2 - Set the Signal-to-Noise Ratio**\n",
        "\n",
        "      * We'll start by cleaning the data. The prompt explicitly tells the AI to separate substantive content from conversational noise."
      ],
      "metadata": {
        "id": "40PTjmLaSggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: g2 - Isolating Content from Noise\n",
        "prompt_g2 = f\"\"\"\n",
        "        Analyze the following meeting transcript. Your task is to isolate the substantive content from the conversational noise.\n",
        "        - Substantive content includes: decisions made, project updates, problems raised, and strategic suggestions.\n",
        "        - Noise includes: greetings, pleasantries, and off-topic remarks (like coffee).\n",
        "        Return ONLY the substantive content.\n",
        "\n",
        "        Transcript:\n",
        "        ---\n",
        "        {meeting_transcript}\n",
        "        ---\n",
        "        \"\"\""
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "069eAXt9SggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "try:\n",
        "    client = OpenAI()\n",
        "\n",
        "    response_g2 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt_g2}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    substantive_content = response_g2.choices[0].message.content\n",
        "    print(\"--- SUBSTANTIVE CONTENT ---\")\n",
        "    print(substantive_content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptfVZHlps3r4",
        "outputId": "08f9813f-3457-4b42-ced3-b9b5931b91d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SUBSTANTIVE CONTENT ---\n",
            "- Decisions:\n",
            "  - Tom will work over the weekend to avoid a 3-day slip from the payment gateway integration snag (tentative).\n",
            "  - Launch strategy: soft launch (invite-only) for the first week; delay the big social media push until after the soft launch.\n",
            "\n",
            "- Project updates:\n",
            "  - Backend is mostly on track; payment gateway integration may require an additional three days.\n",
            "  - Front-end is ahead of schedule and has extra bandwidth.\n",
            "\n",
            "- Problems/Risks:\n",
            "  - Payment gateway integration is more complex than documentation suggests, risking schedule and compressing the final testing buffer.\n",
            "  - A large day-one marketing push could overload servers if initial bugs exist.\n",
            "\n",
            "- Strategic suggestions:\n",
            "  - Use a soft launch to control load and address early issues before broad marketing (proposed by Maria; approved).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 5: g3 - Define the Scope of Time (Simulated RAG)**\n",
        "\n",
        "      * We'll simulate a RAG context by providing a \"previous\" summary and asking for what's new. This teaches the user the importance of historical context."
      ],
      "metadata": {
        "id": "XY2ZlN8sSggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: g3 - Identifying NEW Information (Simulated RAG)\n",
        "previous_summary = \"In our last meeting, we finalized the goals for Project Phoenix and assigned backend work to Tom and front-end to Maria.\"\n",
        "\n",
        "prompt_g3 = f\"\"\"\n",
        "Context: The summary of our last meeting was: \"{previous_summary}\"\n",
        "\n",
        "Task: Analyze the following substantive content from our new meeting. Identify and summarize ONLY the new developments, problems, or decisions that have occurred since the last meeting.\n",
        "\n",
        "New Meeting Content:\n",
        "---\n",
        "{substantive_content}\n",
        "---\n",
        "\"\"\"\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Your chat completion request\n",
        "try:\n",
        "    response_g3 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g3}]\n",
        "    )\n",
        "    new_developments = response_g3.choices[0].message.content\n",
        "    print(\"--- NEW DEVELOPMENTS SINCE LAST MEETING ---\")\n",
        "    print(new_developments)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NEW DEVELOPMENTS SINCE LAST MEETING ---\n",
            "New decisions:\n",
            "- Adopt a soft launch (invite-only) for the first week; delay the major social media push until after the soft launch (approved).\n",
            "- Tom may work over the weekend to mitigate a potential 3-day delay from payment gateway integration (tentative).\n",
            "\n",
            "New developments/updates:\n",
            "- Backend is largely on track, but payment gateway integration may add three days.\n",
            "- Front-end is ahead of schedule and has extra bandwidth.\n",
            "\n",
            "New problems/risks:\n",
            "- Payment gateway integration is more complex than expected, risking schedule and compressing final testing time.\n",
            "- A large day-one marketing push could overload servers if early bugs exist (mitigated by the soft launch decision).\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "id": "1LhMAk4-SggH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c00bec9-ed01-4005-ec46-9d7c90f53372"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 2: Conducting the Investigation (The 'How')\n",
        "\n",
        "Now we move from identifying facts to generating insights, the core of the semantic context interpreation journey.\n",
        "\n",
        "1.  **Cell 6: g4 - Identify the Key Threads**\n",
        "\n",
        "      * This is a crucial step. The prompt asks the AI to read between the lines."
      ],
      "metadata": {
        "id": "wh2UWr4ySggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: g4 - Uncovering Implicit Threads\n",
        "prompt_g4 = f\"\"\"\n",
        "Task: Analyze the following meeting content for implicit social dynamics and unstated feelings. Go beyond the literal words.\n",
        "- Did anyone seem hesitant or reluctant despite agreeing to something?\n",
        "- Were there any underlying disagreements or tensions?\n",
        "- What was the overall mood?\n",
        "\n",
        "Meeting Content:\n",
        "---\n",
        "{substantive_content}\n",
        "---\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g4 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g4}]\n",
        "    )\n",
        "    implicit_threads = response_g4.choices[0].message.content\n",
        "    print(\"--- IMPLICIT THREADS AND DYNAMICS ---\")\n",
        "    print(implicit_threads)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- IMPLICIT THREADS AND DYNAMICS ---\n",
            "Here’s what seems to be happening beneath the surface:\n",
            "\n",
            "Hesitancy or reluctance\n",
            "- Tom’s weekend work is marked “tentative,” suggesting he hasn’t fully committed or is reluctant. It reads like a pressured concession to protect the schedule rather than an enthusiastic choice.\n",
            "- The team implicitly accepts reduced testing time, but the repeated emphasis on risk hints at discomfort with this trade-off.\n",
            "- The delayed big marketing push likely has reluctant buy-in from anyone focused on momentum/visibility; it’s framed as necessary, not desirable.\n",
            "\n",
            "Underlying disagreements or tensions\n",
            "- Speed vs quality: Compressing the final testing buffer to hit dates conflicts with engineering risk concerns. The soft launch is a compromise, but the tension remains.\n",
            "- Uneven load: Front-end has “extra bandwidth,” while backend (Tom) is taking weekend work. The absence of a plan to reallocate FE capacity to help BE/integration suggests a quiet imbalance that could breed frustration.\n",
            "- External dependency frustration: The gateway being “more complex than documentation suggests” implies irritation with the vendor and a sense of reduced control.\n",
            "- Marketing vs engineering caution: Choosing a soft launch and delaying the big push suggests engineering risk-management won out; marketing may be acquiescing rather than aligned.\n",
            "- Individual burden vs team solution: Naming Tom as the weekend buffer indicates the risk is being absorbed by one person instead of a team-level plan—potential for resentment or burnout.\n",
            "\n",
            "Overall mood\n",
            "- Pragmatic, cautious, and somewhat tense. The group is aligned on risk mitigation, but there’s an undercurrent of urgency and pressure.\n",
            "- Cooperative on the surface (Maria’s suggestion “approved”), yet with hints of strain around workload fairness and schedule pressure.\n",
            "- Not defeatist—more sober and damage-control oriented, prioritizing stability over splash.\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "id": "Tkb3x_cnSggI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d516411-a2e9-4609-b23c-3daf5cedb74e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 7: g5 - Perform \"Intellectual Combinations\"**\n",
        "\n",
        "      * Here, we prompt the AI to be creative and solve a problem by synthesizing different ideas from the meeting."
      ],
      "metadata": {
        "id": "yyd_EZ5HSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: g5 - Generating a Novel Solution\n",
        "prompt_g5 = f\"\"\"\n",
        "Context: In the meeting, Maria suggested a 'soft launch' to avoid server strain, and also mentioned her team has 'extra bandwidth'.\n",
        "Tom is facing a 3-day delay on the backend.\n",
        "\n",
        "Task: Propose a novel, actionable idea that uses Maria's team's extra bandwidth to help mitigate Tom's 3-day delay. Combine these two separate pieces of information into a single solution.\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g5 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g5}]\n",
        "    )\n",
        "    novel_solution = response_g5.choices[0].message.content\n",
        "    print(\"--- NOVEL SOLUTION PROPOSED BY AI ---\")\n",
        "    print(novel_solution)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NOVEL SOLUTION PROPOSED BY AI ---\n",
            "Idea: Feature-flagged soft launch with a temporary API facade that caches reads and queues writes\n",
            "\n",
            "How it uses Maria’s extra bandwidth\n",
            "- Maria’s team builds a lightweight “backend-for-frontend” (BFF) or edge function layer that sits in front of Tom’s backend.\n",
            "- They also run the soft launch via feature flags to limit traffic while the facade is in place.\n",
            "\n",
            "What it does\n",
            "- Reads: For the endpoints Tom hasn’t finished, the facade serves data from a warmed cache/snapshot (e.g., Redis/edge cache). This makes most GETs work without the new backend.\n",
            "- Writes: For non-critical writes, the facade returns 202 Accepted, puts the request into a queue (SQS/Kafka), and shows “processing” UI. When Tom’s backend is ready, the queue drains and reconciles.\n",
            "- Critical flows (e.g., payments) are excluded from the beta cohort until Tom is live.\n",
            "\n",
            "Action plan (3-day window)\n",
            "- Today (2–3 hours):\n",
            "  - Identify incomplete endpoints and classify: read vs write, critical vs non-critical.\n",
            "  - Set feature flags (e.g., LaunchDarkly) for “soft_launch_beta” cohort (internal + 5–10% users).\n",
            "  - Maria’s team scaffolds the facade (Cloudflare Workers, Fastly Compute@Edge, or API Gateway + Lambda).\n",
            "- Day 1:\n",
            "  - Implement read-through cache for key GETs; warm with a snapshot job.\n",
            "  - Implement queued writes (202 + enqueue) and user-facing status messages.\n",
            "  - Add observability (latency, queue depth, cache hit rate) and alerting.\n",
            "- Day 2:\n",
            "  - QA with internal cohort; tune cache TTLs, retry/backoff for queue consumers.\n",
            "  - Roll out to 5–10% beta users under the feature flag.\n",
            "- Day 3:\n",
            "  - Tom finishes backend; switch the facade to pass-through for completed endpoints.\n",
            "  - Drain the queue and reconcile; gradually raise the feature flag to more users.\n",
            "  - Remove temporary stubs as endpoints go live.\n",
            "\n",
            "Why this mitigates the 3-day delay\n",
            "- The soft launch limits traffic while the facade masks incomplete backend pieces.\n",
            "- Users get a working experience (reads are fast from cache; writes are accepted and processed later).\n",
            "- Tom gets the full 3 days to finish without blocking the launch, and the switch-over is controlled per-endpoint via flags.\n",
            "\n",
            "Notes/guardrails\n",
            "- Exclude any write that must be synchronous or has compliance constraints.\n",
            "- Encrypt queued payloads; make writes idempotent for safe replay.\n",
            "- Provide clear “processing” states and SLAs in the UI for queued actions.\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "id": "8JN5E5X4SggI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56a189b-81f9-4790-f709-99b8b292b21d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 3: Determining the Action (The 'What Next')\n",
        "\n",
        "Finally, we turn the analysis into concrete, forward-looking artifacts.\n",
        "\n",
        "1.  **Cell 8: g6 - Define the Output Format (Final Summary)**\n",
        "\n",
        "      * We compile all the key information into a structured, final summary, showing the importance of clear outputs."
      ],
      "metadata": {
        "id": "J0CD-ztrSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: g6 - Creating the Final, Structured Summary\n",
        "prompt_g6 = f\"\"\"\n",
        "Task: Create a final, concise summary of the meeting in a markdown table.\n",
        "Use the following information to construct the table.\n",
        "\n",
        "- New Developments: {new_developments}\n",
        "\n",
        "The table should have three columns: \"Topic\", \"Decision/Outcome\", and \"Owner\".\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g6 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g6}]\n",
        "    )\n",
        "    final_summary_table = response_g6.choices[0].message.content\n",
        "    print(\"--- FINAL MEETING SUMMARY TABLE ---\")\n",
        "    print(final_summary_table)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FINAL MEETING SUMMARY TABLE ---\n",
            "| Topic | Decision/Outcome | Owner |\n",
            "|---|---|---|\n",
            "| Launch strategy | Soft launch (invite-only) for first week; delay major social media push until after soft launch (approved). Mitigates server overload risk from a large day-one push. | Marketing Team |\n",
            "| Payment gateway integration | Integration more complex than expected; may add ~3 days and compress final testing. Mitigation: Tom may work over the weekend to reduce delay (tentative). | Tom; Backend Team |\n",
            "| Backend status | Largely on track aside from payment gateway risk. | Backend Team |\n",
            "| Front-end status | Ahead of schedule; has extra bandwidth to assist. | Front-end Team |\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "id": "xXRnPWuESggI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1890e7cd-a794-4498-c416-db6202d0d77e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 9: g7 - Generate the Subsequent Task**\n",
        "\n",
        "      * The last step is to use the analysis to perform a real-world action, closing the loop from insight to action."
      ],
      "metadata": {
        "id": "jk1g9wVvSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: g7 - Drafting the Follow-Up Action\n",
        "prompt_g7 = f\"\"\"\n",
        "Task: Based on the following summary table, draft a polite and professional follow-up email to the team (Sarah, Tom, Maria).\n",
        "The email should clearly state the decisions made and the action items for each person.\n",
        "\n",
        "Summary Table:\n",
        "---\n",
        "{final_summary_table}\n",
        "---\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g7 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g7}]\n",
        "    )\n",
        "    follow_up_email = response_g7.choices[0].message.content\n",
        "    print(\"--- DRAFT FOLLOW-UP EMAIL ---\")\n",
        "    print(follow_up_email)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DRAFT FOLLOW-UP EMAIL ---\n",
            "Subject: Follow-up: Launch plan, payment gateway, and next steps\n",
            "\n",
            "Hi Sarah, Tom, and Maria,\n",
            "\n",
            "Thank you for the productive discussion. Here’s a quick recap of decisions and the action items for each of you.\n",
            "\n",
            "Decisions\n",
            "- Launch strategy: We’ll do an invite-only soft launch for the first week. We’ll hold the major social media push until after the soft launch to reduce server overload risk.\n",
            "- Payment gateway: Integration is more complex than expected and may add ~3 days, compressing final testing. Mitigation: Tom may work over the weekend (to be confirmed).\n",
            "- Status: Backend is largely on track aside from the gateway risk. Front-end is ahead of schedule and can assist.\n",
            "\n",
            "Action items\n",
            "- Sarah (Marketing)\n",
            "  - Lead the invite-only soft launch (define audience, finalize invite list/criteria, schedule sends).\n",
            "  - Prepare soft launch comms (invites, landing page updates); draft broader social content but hold until we greenlight post soft launch.\n",
            "  - Coordinate with Support/Analytics to capture feedback and metrics during the soft launch.\n",
            "\n",
            "- Tom (Backend)\n",
            "  - Share a revised payment gateway integration plan and timeline, including test impact and dependencies.\n",
            "  - Confirm whether you can work over the weekend and estimate how much time that would recover.\n",
            "  - Identify specific areas where Front-end can assist (e.g., checkout/payment UI hooks, error states, test instrumentation) and sync with Maria.\n",
            "  - Flag early if the ~3-day slip looks likely so we can adjust the launch calendar.\n",
            "\n",
            "- Maria (Front-end)\n",
            "  - Allocate Front-end bandwidth to support Backend tasks per Tom’s list.\n",
            "  - Implement/verify any gating needed for invite-only access (feature flags/access codes).\n",
            "  - Partner with Tom on test coverage and QA for the compressed window, especially around payment flows and error handling.\n",
            "\n",
            "I’ll consolidate updates and share a brief status summary tomorrow EOD. Please reply with any blockers or changes.\n",
            "\n",
            "Thanks,\n",
            "[Your Name]\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "id": "aQ1PzmTlSggI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1eb124-6f5b-4712-9540-59bb2fd64307"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook structure doesn't just \"get a summary.\" It takes the user on a journey, showing them *how* to think with the AI as a partner. It perfectly translates the abstract \"Scope, Investigation, Action\" framework into a repeatable, educational, and powerful process."
      ],
      "metadata": {
        "id": "-tDIxvHSSggJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}