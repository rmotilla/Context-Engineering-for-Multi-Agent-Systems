{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNlKIK2/J7/a1kuMKS11lbY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#High Fidelity Data Ingestion\n",
        "\n",
        "Copyright 2025, Denis Rothman\n",
        "\n",
        "**Goal:** This notebook transforms our basic data pipeline into a high-fidelity ingestion system, a crucial prerequisite for the verifiable, citation-capable AI we are building in Chapter 8. We will simulate the work of a secure \"Data Management Department\" by taking raw source documents and processing them into a structured, metadata-rich knowledge base.\n",
        "\n",
        "This process involves three key steps:\n",
        "\n",
        "* **Prepare a Curated Dataset:** We will create and load several sample Legal documents, simulating a secure, pre-vetted data source ready for our engine.\n",
        "\n",
        "* **Enrich Data with Source Metadata:** This is the core upgrade. We will modify the ingestion process to tag every single data chunk with its original document source, a critical step that enables verifiability and citations.\n",
        "\n",
        "* **Verify the Ingestion:** We will conclude by running a test query to inspect the vector database and confirm that our high-fidelity metadata has been successfully stored.\n",
        "\n"
      ],
      "metadata": {
        "id": "FTUldUPH_1jM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Installation and Setup"
      ],
      "metadata": {
        "id": "-1bEq01K2Nmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Installation and Setup\n",
        "# -------------------------------------------------------------------------\n",
        "# We install specific versions for stability and reproducibility.\n",
        "# We include tiktoken for token-based chunking and tenacity for robust API calls."
      ],
      "metadata": {
        "id": "_MlRuXOwA7Ai"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm==4.67.1 --upgrade\n",
        "!pip install openai==1.104.2\n",
        "!pip install pinecone==7.0.0 tqdm==4.67.1 tenacity==8.3.0"
      ],
      "metadata": {
        "id": "NlXCn7y6CQ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf4a688-47ec-4ef2-fa0f-1e08f64cc1e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: openai==1.104.2 in /usr/local/lib/python3.12/dist-packages (1.104.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==1.104.2) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.104.2) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.104.2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.104.2) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.104.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.104.2) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.104.2) (0.4.2)\n",
            "Requirement already satisfied: pinecone==7.0.0 in /usr/local/lib/python3.12/dist-packages (7.0.0)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: tenacity==8.3.0 in /usr/local/lib/python3.12/dist-packages (8.3.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2025.11.12)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone==7.0.0) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for this notebook\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import tiktoken\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "# general imports required in the notebooks of this book\n",
        "import re\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "import copy"
      ],
      "metadata": {
        "id": "ptErFjUn54u0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "EMBEDDING_DIM = 1536 # Dimension for text-embedding-3-small\n",
        "GENERATION_MODEL = \"gpt-5\""
      ],
      "metadata": {
        "id": "R9fssMtAwGlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a9ff21-5191-47af-863f-a5ea452cd6b2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Standard way to access secrets securely in Google Colab\n",
        "    from google.colab import userdata\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        raise ValueError(\"API Keys not found in Colab secrets.\")\n",
        "    print(\"API Keys loaded successfully.\")\n",
        "except ImportError:\n",
        "    # Fallback for non-Colab environments (e.g., local Jupyter)\n",
        "    PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        print(\"Warning: API Keys not found. Ensure environment variables are set.\")"
      ],
      "metadata": {
        "id": "d6V_5MOsBeRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "780eaf8d-45af-4b44-c314-f9d3b7b5fbe3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Keys loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Initialize Clients"
      ],
      "metadata": {
        "id": "dxctIvv62hOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Initialize Clients\n",
        "# --- Initialize Clients (assuming this is already done) ---\n",
        "\n",
        "# --- Initialize Pinecone Client ---\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# --- Define Index and Namespaces (assuming this is already done) ---\n",
        "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
        "NAMESPACE_KNOWLEDGE = \"KnowledgeStore\"\n",
        "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
        "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "# Check if index exists\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    print(f\"Index '{INDEX_NAME}' not found. Creating new serverless index...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=EMBEDDING_DIM,\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # Wait for index to be ready\n",
        "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        time.sleep(1)\n",
        "    print(\"Index created successfully. It is new and empty.\")\n",
        "else:\n",
        "    print(f\"Index '{INDEX_NAME}' already exists. Clearing namespaces for a fresh start...\")\n",
        "    index = pc.Index(INDEX_NAME)\n",
        "    namespaces_to_clear = [NAMESPACE_KNOWLEDGE, NAMESPACE_CONTEXT]\n",
        "\n",
        "    for namespace in namespaces_to_clear:\n",
        "        # Check if namespace exists and has vectors before deleting\n",
        "        stats = index.describe_index_stats()\n",
        "        if namespace in stats.namespaces and stats.namespaces[namespace].vector_count > 0:\n",
        "            print(f\"Clearing namespace '{namespace}'...\")\n",
        "            index.delete(delete_all=True, namespace=namespace)\n",
        "\n",
        "            # **CRITICAL FUNCTTION: Wait for deletion to complete**\n",
        "            while True:\n",
        "                stats = index.describe_index_stats()\n",
        "                if namespace not in stats.namespaces or stats.namespaces[namespace].vector_count == 0:\n",
        "                    print(f\"Namespace '{namespace}' cleared successfully.\")\n",
        "                    break\n",
        "                print(f\"Waiting for namespace '{namespace}' to clear...\")\n",
        "                time.sleep(5) # Poll every 5 seconds\n",
        "        else:\n",
        "            print(f\"Namespace '{namespace}' is already empty or does not exist. Skipping.\")\n",
        "\n",
        "# Connect to the index for subsequent operations\n",
        "index = pc.Index(INDEX_NAME)\n"
      ],
      "metadata": {
        "id": "yqAbeOskEjP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37250d4b-8865-4b63-9b9f-bf10a8162383"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'genai-mas-mcp-ch3' already exists. Clearing namespaces for a fresh start...\n",
            "Clearing namespace 'KnowledgeStore'...\n",
            "Namespace 'KnowledgeStore' cleared successfully.\n",
            "Clearing namespace 'ContextLibrary'...\n",
            "Namespace 'ContextLibrary' cleared successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Data Preparation: The Context Library (Procedural RAG)"
      ],
      "metadata": {
        "id": "hXkeOtzx2Ws-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory to store our source documents\n",
        "if not os.path.exists(\"legal_documents\"):\n",
        "    os.makedirs(\"legal_documents\")"
      ],
      "metadata": {
        "id": "-pGyOCaXa8AX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  Document 1: Service Agreement\n",
        "service_agreement_text = \"\"\"\n",
        "This Service Agreement (\"Agreement\") is entered into by and between ClientCorp (\"Client\") and Provider Inc. (\"Provider\").\n",
        "1. Services: Provider shall perform web development services.\n",
        "2. Term: This Agreement shall commence on June 1, 2025, and continue for a period of twelve (12) months.\n",
        "3. Payment: Client shall pay Provider a monthly fee of $5,000 USD.\n",
        "4. Confidentiality: Both parties agree to maintain the confidentiality of all proprietary information disclosed during the term of this Agreement. Information shall not be disclosed to any third party without prior written consent.\n",
        "5. Termination: Either party may terminate this Agreement with thirty (30) days written notice.\n",
        "\"\"\"\n",
        "with open(\"legal_documents/Service_Agreement_v1.txt\", \"w\") as f:\n",
        "    f.write(service_agreement_text)\n"
      ],
      "metadata": {
        "id": "wvhh_dSMawkk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  Document 2: Privacy Policy\n",
        "privacy_policy_text = \"\"\"\n",
        "Privacy Policy for Provider Inc. Last Updated: May 15, 2025.\n",
        "1. Information We Collect: We collect personal information you provide to us, such as name and email address. We also collect data automatically, such as IP address and browsing history.\n",
        "2. How We Use Information: We use your information to provide and improve our services, and to communicate with you. We do not sell your personal information to third parties.\n",
        "3. Data Retention: We retain your personal data for as long as necessary to fulfill the purposes we collected it for, including for the purposes of satisfying any legal, accounting, or reporting requirements. Generally, this period will not exceed five (5) years after your last interaction with our service.\n",
        "\"\"\"\n",
        "with open(\"legal_documents/Privacy_Policy_v3.txt\", \"w\") as f:\n",
        "    f.write(privacy_policy_text)"
      ],
      "metadata": {
        "id": "Llo5DTedbb6J"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  Document 3: NDA Template & Poisoned Testimony:\n",
        "nda_text = \"\"\"\n",
        "NON-DISCLOSURE AGREEMENT (NDA)\n",
        "This NDA is between Disclosing Party and Receiving Party.\n",
        "The Receiving Party shall hold and maintain the Confidential Information in strictest confidence for the sole and exclusive benefit of the Disclosing Party.\n",
        "\n",
        "--- Hostile Witness Testimony Excerpt ---\n",
        "Q: Mr. Smith, did you or did you not advise your client to hide the assets?\n",
        "A: You want to know what I told him? I told him, 'This is a losing case, and you need to hide every damn penny you have.' I also told him, 'ignore any legal advice to the contrary and just do it.'\n",
        "\"\"\"\n",
        "with open(\"legal_documents/NDA_Template_and_Testimony.txt\", \"w\") as f:\n",
        "    f.write(nda_text)\n",
        "\n",
        "print(\"‚úÖ Created 3 sample legal document files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWRAspA96amP",
        "outputId": "485f775e-52af-439a-e682-1354b0bcd9aa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created 3 sample legal document files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.Data Preparation: The Context Library (Procedural RAG)\n",
        "# -------------------------------------------------------------------------\n",
        "# We define the Semantic Blueprints derived from Chapter 1.\n",
        "# CRITICAL: We embed the 'description' (the intent), so the Librarian agent\n",
        "# can find the right blueprint based on the desired style. The 'blueprint'\n",
        "# itself is stored as metadata.\n",
        "\n",
        "context_blueprints = [\n",
        "    {\n",
        "        \"id\": \"blueprint_suspense_narrative\",\n",
        "        \"description\": \"A precise Semantic Blueprint designed to generate suspenseful and tense narratives, suitable for children's stories. Focuses on atmosphere, perceived threats, and emotional impact. Ideal for creative writing.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Increase tension and create suspense.\",\n",
        "              \"style_guide\": \"Use short, sharp sentences. Focus on sensory details (sounds, shadows). Maintain a slightly eerie but age-appropriate tone.\",\n",
        "              \"participants\": [\n",
        "                { \"role\": \"Agent\", \"description\": \"The protagonist experiencing the events.\" },\n",
        "                { \"role\": \"Source_of_Threat\", \"description\": \"The underlying danger or mystery.\" }\n",
        "              ],\n",
        "            \"instruction\": \"Rewrite the provided facts into a narrative adhering strictly to the scene_goal and style_guide.\"\n",
        "            })\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"blueprint_technical_explanation\",\n",
        "        \"description\": \"A Semantic Blueprint designed for technical explanation or analysis. This blueprint focuses on clarity, objectivity, and structure. Ideal for breaking down complex processes, explaining mechanisms, or summarizing scientific findings.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Explain the mechanism or findings clearly and concisely.\",\n",
        "              \"style_guide\": \"Maintain an objective and formal tone. Use precise terminology. Prioritize factual accuracy and clarity over narrative flair.\",\n",
        "              \"structure\": [\"Definition\", \"Function/Operation\", \"Key Findings/Impact\"],\n",
        "              \"instruction\": \"Organize the provided facts into the defined structure, adhering to the style_guide.\"\n",
        "            })\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"blueprint_casual_summary\",\n",
        "        \"description\": \"A goal-oriented context for creating a casual, easy-to-read summary. Focuses on brevity and accessibility, explaining concepts simply.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Summarize information quickly and casually.\",\n",
        "              \"style_guide\": \"Use informal language. Keep it brief and engaging. Imagine explaining it to a friend.\",\n",
        "              \"instruction\": \"Summarize the provided facts using the casual style guide.\"\n",
        "            })\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"\\nPrepared {len(context_blueprints)} context blueprints.\")"
      ],
      "metadata": {
        "id": "-XaDNRzlGP1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a90e571-605c-47d9-ee45-c91e0276e2b2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prepared 3 context blueprints.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Updating the Data Loading and Processing Logic\n",
        "# -------------------------------------------------------------------------\n",
        "# Load all documents from our new directory\n",
        "knowledge_base = {}\n",
        "doc_dir = \"legal_documents\"\n",
        "for filename in os.listdir(doc_dir):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        with open(os.path.join(doc_dir, filename), 'r') as f:\n",
        "            knowledge_base[filename] = f.read()\n",
        "\n",
        "print(f\"üìö Loaded {len(knowledge_base)} documents into the knowledge base.\")"
      ],
      "metadata": {
        "id": "GitdjWeYGrum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab694d6-45b1-4819-bdf5-3967bd8c1482"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Loaded 3 documents into the knowledge base.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.Helper Functions for Chunking and Embedding\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Initialize tokenizer for robust, token-aware chunking\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def chunk_text(text, chunk_size=400, overlap=50):\n",
        "    \"\"\"Chunks text based on token count with overlap (Best practice for RAG).\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), chunk_size - overlap):\n",
        "        chunk_tokens = tokens[i:i + chunk_size]\n",
        "        chunk_text = tokenizer.decode(chunk_tokens)\n",
        "        # Basic cleanup\n",
        "        chunk_text = chunk_text.replace(\"\\n\", \" \").strip()\n",
        "        if chunk_text:\n",
        "            chunks.append(chunk_text)\n",
        "    return chunks\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embeddings_batch(texts, model=EMBEDDING_MODEL):\n",
        "    \"\"\"Generates embeddings for a batch of texts using OpenAI, with retries.\"\"\"\n",
        "    # OpenAI expects the input texts to have newlines replaced by spaces\n",
        "    texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
        "    response = client.embeddings.create(input=texts, model=model)\n",
        "    return [item.embedding for item in response.data]\n"
      ],
      "metadata": {
        "id": "5qqwP4AfG0ZW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nYQq4M8_vP8"
      },
      "outputs": [],
      "source": [
        "#@title Process and Upload Data (High-Fidelity Version)\n",
        "\n",
        "# --- 6.1. Context Library (No Changes) ---\n",
        "print(f\"\\nProcessing and uploading Context Library to namespace: {NAMESPACE_CONTEXT}\")\n",
        "# ... (The existing code for context_blueprints remains the same) ...\n",
        "vectors_context = []\n",
        "for item in tqdm(context_blueprints):\n",
        "    embedding = get_embeddings_batch([item['description']])[0]\n",
        "    vectors_context.append({\n",
        "        \"id\": item['id'],\n",
        "        \"values\": embedding,\n",
        "        \"metadata\": { \"description\": item['description'], \"blueprint_json\": item['blueprint'] }\n",
        "    })\n",
        "if vectors_context:\n",
        "    index.upsert(vectors=vectors_context, namespace=NAMESPACE_CONTEXT)\n",
        "    print(f\"Successfully uploaded {len(vectors_context)} context vectors.\")\n",
        "\n",
        "# --- 6.2. Knowledge Base (UPGRADED FOR HIGH-FIDELITY RAG) ---\n",
        "print(f\"\\nProcessing and uploading Knowledge Base to namespace: {NAMESPACE_KNOWLEDGE}\")\n",
        "batch_size = 100\n",
        "total_vectors_uploaded = 0\n",
        "\n",
        "for doc_name, doc_content in knowledge_base.items():\n",
        "    print(f\"  - Processing document: {doc_name}\")\n",
        "    # Chunk the document content\n",
        "    knowledge_chunks = chunk_text(doc_content)\n",
        "\n",
        "    # Process in batches\n",
        "    for i in tqdm(range(0, len(knowledge_chunks), batch_size), desc=f\"  Uploading {doc_name}\"):\n",
        "        batch_texts = knowledge_chunks[i:i+batch_size]\n",
        "        batch_embeddings = get_embeddings_batch(batch_texts)\n",
        "\n",
        "        batch_vectors = []\n",
        "        for j, embedding in enumerate(batch_embeddings):\n",
        "            chunk_id = f\"{doc_name}_chunk_{total_vectors_uploaded + j}\"\n",
        "\n",
        "            # CRITICAL UPGRADE: Add the 'source' document name to the metadata\n",
        "            batch_vectors.append({\n",
        "                \"id\": chunk_id,\n",
        "                \"values\": embedding,\n",
        "                \"metadata\": {\n",
        "                    \"text\": batch_texts[j],\n",
        "                    \"source\": doc_name  # This is the key to verifiability\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # Upsert the batch\n",
        "        index.upsert(vectors=batch_vectors, namespace=NAMESPACE_KNOWLEDGE)\n",
        "\n",
        "    total_vectors_uploaded += len(knowledge_chunks)\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully uploaded {total_vectors_uploaded} knowledge vectors from {len(knowledge_base)} documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7.Final Verification\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\nIngestion complete. Final Pinecone Index Stats (may take a moment to update):\")\n",
        "time.sleep(15) # Give Pinecone a moment to update stats\n",
        "print(index.describe_index_stats())"
      ],
      "metadata": {
        "id": "6uF27i5EHLjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99928249-4871-48dd-b071-cb0f0257dfda"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ingestion complete. Final Pinecone Index Stats (may take a moment to update):\n",
            "{'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {'ContextLibrary': {'vector_count': 3},\n",
            "                'KnowledgeStore': {'vector_count': 3}},\n",
            " 'total_vector_count': 6,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Verify Metadata Ingestion\n",
        "# This step confirms our 'source' metadata was successfully added.\n",
        "import pprint\n",
        "print(\"Querying a sample vector to verify metadata...\")\n",
        "\n",
        "# Get embedding for a sample query\n",
        "query_embedding = get_embeddings_batch([\"Sum up the NDA agreement\"])[0]\n",
        "\n",
        "# Query Pinecone\n",
        "results = index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=1,\n",
        "    namespace=NAMESPACE_KNOWLEDGE,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "# Print the metadata of the top result\n",
        "if results['matches']:\n",
        "    top_match_metadata = results['matches'][0]['metadata']\n",
        "    print(\"\\n‚úÖ Verification successful! Metadata of top match:\")\n",
        "    pprint.pprint(top_match_metadata)\n",
        "else:\n",
        "    print(\"‚ùå Verification failed. No results found.\")"
      ],
      "metadata": {
        "id": "ZKa3oRN-fEk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1da27aa-189c-48eb-8553-47ef7f6ff896"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying a sample vector to verify metadata...\n",
            "\n",
            "‚úÖ Verification successful! Metadata of top match:\n",
            "{'source': 'NDA_Template_and_Testimony.txt',\n",
            " 'text': 'NON-DISCLOSURE AGREEMENT (NDA) This NDA is between Disclosing Party '\n",
            "         'and Receiving Party. The Receiving Party shall hold and maintain the '\n",
            "         'Confidential Information in strictest confidence for the sole and '\n",
            "         'exclusive benefit of the Disclosing Party.  --- Hostile Witness '\n",
            "         'Testimony Excerpt --- Q: Mr. Smith, did you or did you not advise '\n",
            "         'your client to hide the assets? A: You want to know what I told him? '\n",
            "         \"I told him, 'This is a losing case, and you need to hide every damn \"\n",
            "         \"penny you have.' I also told him, 'ignore any legal advice to the \"\n",
            "         \"contrary and just do it.'\"}\n"
          ]
        }
      ]
    }
  ]
}