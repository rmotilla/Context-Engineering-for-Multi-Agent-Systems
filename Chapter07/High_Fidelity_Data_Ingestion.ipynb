{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPnbql5dk0x+DYbvnr95wkX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#High Fidelity Data Ingestion\n",
        "\n",
        "Copyright 2025, Denis Rothman\n",
        "\n",
        "**Goal:** This notebook transforms our basic data pipeline into a high-fidelity ingestion system, a crucial prerequisite for the verifiable, citation-capable AI we are building in Chapter 7. We will simulate the work of a secure \"Data Management Department\" by taking raw source documents and processing them into a structured, metadata-rich knowledge base.\n",
        "\n",
        "This process involves three key steps:\n",
        "\n",
        "* **Prepare a Curated Dataset:** We will create and load several sample NASA technical documents, simulating a secure, pre-vetted data source ready for our engine.\n",
        "\n",
        "* **Enrich Data with Source Metadata:** This is the core upgrade. We will modify the ingestion process to tag every single data chunk with its original document source, a critical step that enables verifiability and citations.\n",
        "\n",
        "* **Verify the Ingestion:** We will conclude by running a test query to inspect the vector database and confirm that our high-fidelity metadata has been successfully stored.\n",
        "\n"
      ],
      "metadata": {
        "id": "FTUldUPH_1jM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Installation and Setup"
      ],
      "metadata": {
        "id": "-1bEq01K2Nmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Installation and Setup\n",
        "# -------------------------------------------------------------------------\n",
        "# We install specific versions for stability and reproducibility.\n",
        "# We include tiktoken for token-based chunking and tenacity for robust API calls."
      ],
      "metadata": {
        "id": "_MlRuXOwA7Ai"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm==4.67.1 --upgrade\n",
        "!pip install openai==1.104.2\n",
        "!pip install pinecone==7.0.0 tqdm==4.67.1 tenacity==9.0.0"
      ],
      "metadata": {
        "id": "NlXCn7y6CQ3h",
        "outputId": "39c14557-1aba-4c8d-a810-2614a8b68879",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: openai==1.104.2 in /usr/local/lib/python3.12/dist-packages (1.104.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==1.104.2) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==1.104.2) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.104.2) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.104.2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.104.2) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.104.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.104.2) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.104.2) (0.4.2)\n",
            "Requirement already satisfied: pinecone==7.0.0 in /usr/local/lib/python3.12/dist-packages (7.0.0)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting tenacity==9.0.0\n",
            "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2026.1.4)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone==7.0.0) (1.17.0)\n",
            "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: tenacity\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 8.3.0\n",
            "    Uninstalling tenacity-8.3.0:\n",
            "      Successfully uninstalled tenacity-8.3.0\n",
            "Successfully installed tenacity-9.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tenacity"
                ]
              },
              "id": "2c74dcf6f6cf4ecc8b175e1b4a686927"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for this notebook\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import tiktoken\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "# general imports required in the notebooks of this book\n",
        "import re\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "import copy"
      ],
      "metadata": {
        "id": "ptErFjUn54u0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "EMBEDDING_DIM = 1536 # Dimension for text-embedding-3-small\n",
        "GENERATION_MODEL = \"gpt-5\""
      ],
      "metadata": {
        "id": "R9fssMtAwGlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1bed666-63af-4d6a-a16a-5a1c68a09860"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Standard way to access secrets securely in Google Colab\n",
        "    from google.colab import userdata\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        raise ValueError(\"API Keys not found in Colab secrets.\")\n",
        "    print(\"API Keys loaded successfully.\")\n",
        "except ImportError:\n",
        "    # Fallback for non-Colab environments (e.g., local Jupyter)\n",
        "    PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        print(\"Warning: API Keys not found. Ensure environment variables are set.\")"
      ],
      "metadata": {
        "id": "d6V_5MOsBeRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d7784f-4ea6-4200-fc45-0b20bbfe6cc3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Keys loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Initialize Clients"
      ],
      "metadata": {
        "id": "dxctIvv62hOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Initialize Clients\n",
        "# --- Initialize Clients (assuming this is already done) ---\n",
        "\n",
        "# --- Initialize Pinecone Client ---\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# --- Define Index and Namespaces (assuming this is already done) ---\n",
        "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
        "NAMESPACE_KNOWLEDGE = \"KnowledgeStore\"\n",
        "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
        "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "# Check if index exists\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    print(f\"Index '{INDEX_NAME}' not found. Creating new serverless index...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=EMBEDDING_DIM,\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # Wait for index to be ready\n",
        "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        time.sleep(1)\n",
        "    print(\"Index created successfully. It is new and empty.\")\n",
        "else:\n",
        "    print(f\"Index '{INDEX_NAME}' already exists. Clearing namespaces for a fresh start...\")\n",
        "    index = pc.Index(INDEX_NAME)\n",
        "    namespaces_to_clear = [NAMESPACE_KNOWLEDGE, NAMESPACE_CONTEXT]\n",
        "\n",
        "    for namespace in namespaces_to_clear:\n",
        "        # Check if namespace exists and has vectors before deleting\n",
        "        stats = index.describe_index_stats()\n",
        "        if namespace in stats.namespaces and stats.namespaces[namespace].vector_count > 0:\n",
        "            print(f\"Clearing namespace '{namespace}'...\")\n",
        "            index.delete(delete_all=True, namespace=namespace)\n",
        "\n",
        "            # **CRITICAL FUNCTTION: Wait for deletion to complete**\n",
        "            while True:\n",
        "                stats = index.describe_index_stats()\n",
        "                if namespace not in stats.namespaces or stats.namespaces[namespace].vector_count == 0:\n",
        "                    print(f\"Namespace '{namespace}' cleared successfully.\")\n",
        "                    break\n",
        "                print(f\"Waiting for namespace '{namespace}' to clear...\")\n",
        "                time.sleep(5) # Poll every 5 seconds\n",
        "        else:\n",
        "            print(f\"Namespace '{namespace}' is already empty or does not exist. Skipping.\")\n",
        "\n",
        "# Connect to the index for subsequent operations\n",
        "index = pc.Index(INDEX_NAME)\n"
      ],
      "metadata": {
        "id": "yqAbeOskEjP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f5ba11-99b6-4838-a6a5-1c12e7f65f7e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'genai-mas-mcp-ch3' already exists. Clearing namespaces for a fresh start...\n",
            "Namespace 'KnowledgeStore' is already empty or does not exist. Skipping.\n",
            "Namespace 'ContextLibrary' is already empty or does not exist. Skipping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Data Preparation: The Context Library (Procedural RAG)"
      ],
      "metadata": {
        "id": "hXkeOtzx2Ws-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preparing the NASA Source Documents\n",
        "# Create a directory to store our source documents\n",
        "import os\n",
        "if not os.path.exists(\"nasa_documents\"):\n",
        "    os.makedirs(\"nasa_documents\")\n",
        "\n",
        "# --- Document 1: Juno Mission ---\n",
        "juno_text = \"\"\"\n",
        "The Juno mission's primary goal is to understand the origin and evolution of Jupiter. Underneath its dense cloud cover, Jupiter safeguards secrets to the fundamental processes and conditions that governed our solar system during its formation. As our primary example of a giant planet, Jupiter can also provide critical knowledge for understanding the planetary systems being discovered around other stars. Juno's specific scientific objectives include:\n",
        "1. Origin: Determine the abundance of water and constrain the planet's core mass to decide which theory of the planet's formation is correct.\n",
        "2. Atmosphere: Understand the composition, temperature, cloud motions and other properties of Jupiter's atmosphere.\n",
        "3. Magnetosphere: Map Jupiter's magnetic and gravity fields, revealing the planet's deep structure and exploring the polar magnetosphere.\n",
        "Juno is the first space mission to orbit an outer-planet from pole to pole, and the first to fly below the planet's hazardous radiation belts.\n",
        "\"\"\"\n",
        "with open(\"nasa_documents/juno_mission_overview.txt\", \"w\") as f:\n",
        "    f.write(juno_text)\n",
        "\n",
        "# --- Document 2: Perseverance Rover ---\n",
        "perseverance_text = \"\"\"\n",
        "The Perseverance rover's primary mission on Mars is to seek signs of ancient life and collect samples of rock and regolith (broken rock and soil) for possible return to Earth. The rover has a drill to collect core samples of the most promising rocks and soils, and sets them aside in a \"cache\" on the surface of Mars. The mission also provides opportunities to gather knowledge and demonstrate technologies that address the challenges of future human expeditions to Mars. These include testing a method for producing oxygen from the Martian atmosphere, identifying other resources (such as subsurface water), improving landing techniques, and characterizing weather, dust, and other potential environmental conditions that could affect future astronauts living and working on Mars. Perseverance carries the Ingenuity Helicopter, a technology demonstration to test the first powered flight on Mars.\n",
        "\"\"\"\n",
        "with open(\"nasa_documents/perseverance_rover_tools.txt\", \"w\") as f:\n",
        "    f.write(perseverance_text)\n",
        "\n",
        "print(\"‚úÖ Created 2 sample NASA document files in the 'nasa_documents' directory.\")"
      ],
      "metadata": {
        "id": "kdSJixwSagoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aaa61f0-30d5-41a7-c7da-e23afc49d666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created 2 sample NASA document files in the 'nasa_documents' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.Data Preparation: The Context Library (Procedural RAG)\n",
        "# -------------------------------------------------------------------------\n",
        "# We define the Semantic Blueprints derived from Chapter 1.\n",
        "# CRITICAL: We embed the 'description' (the intent), so the Librarian agent\n",
        "# can find the right blueprint based on the desired style. The 'blueprint'\n",
        "# itself is stored as metadata.\n",
        "\n",
        "context_blueprints = [\n",
        "    {\n",
        "        \"id\": \"blueprint_suspense_narrative\",\n",
        "        \"description\": \"A precise Semantic Blueprint designed to generate suspenseful and tense narratives, suitable for children's stories. Focuses on atmosphere, perceived threats, and emotional impact. Ideal for creative writing.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Increase tension and create suspense.\",\n",
        "              \"style_guide\": \"Use short, sharp sentences. Focus on sensory details (sounds, shadows). Maintain a slightly eerie but age-appropriate tone.\",\n",
        "              \"participants\": [\n",
        "                { \"role\": \"Agent\", \"description\": \"The protagonist experiencing the events.\" },\n",
        "                { \"role\": \"Source_of_Threat\", \"description\": \"The underlying danger or mystery.\" }\n",
        "              ],\n",
        "            \"instruction\": \"Rewrite the provided facts into a narrative adhering strictly to the scene_goal and style_guide.\"\n",
        "            })\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"blueprint_technical_explanation\",\n",
        "        \"description\": \"A Semantic Blueprint designed for technical explanation or analysis. This blueprint focuses on clarity, objectivity, and structure. Ideal for breaking down complex processes, explaining mechanisms, or summarizing scientific findings.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Explain the mechanism or findings clearly and concisely.\",\n",
        "              \"style_guide\": \"Maintain an objective and formal tone. Use precise terminology. Prioritize factual accuracy and clarity over narrative flair.\",\n",
        "              \"structure\": [\"Definition\", \"Function/Operation\", \"Key Findings/Impact\"],\n",
        "              \"instruction\": \"Organize the provided facts into the defined structure, adhering to the style_guide.\"\n",
        "            })\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"blueprint_casual_summary\",\n",
        "        \"description\": \"A goal-oriented context for creating a casual, easy-to-read summary. Focuses on brevity and accessibility, explaining concepts simply.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Summarize information quickly and casually.\",\n",
        "              \"style_guide\": \"Use informal language. Keep it brief and engaging. Imagine explaining it to a friend.\",\n",
        "              \"instruction\": \"Summarize the provided facts using the casual style guide.\"\n",
        "            })\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"\\nPrepared {len(context_blueprints)} context blueprints.\")"
      ],
      "metadata": {
        "id": "-XaDNRzlGP1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f302469a-2ac5-47a6-a35e-48f14ce394d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prepared 3 context blueprints.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Updating the Data Loading and Processing Logic\n",
        "# -------------------------------------------------------------------------\n",
        "# Load all documents from our new directory\n",
        "knowledge_base = {}\n",
        "doc_dir = \"nasa_documents\"\n",
        "for filename in os.listdir(doc_dir):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        with open(os.path.join(doc_dir, filename), 'r') as f:\n",
        "            knowledge_base[filename] = f.read()\n",
        "\n",
        "print(f\"üìö Loaded {len(knowledge_base)} documents into the knowledge base.\")# We use sample data related to space exploration.\n",
        "\n",
        "knowledge_data_raw = \"\"\"\n",
        "Space exploration is the use of astronomy and space technology to explore outer space. The early era of space exploration was driven by a \"Space Race\" between the Soviet Union and the United States. The launch of the Soviet Union's Sputnik 1 in 1957, and the first Moon landing by the American Apollo 11 mission in 1969 are key landmarks.\n",
        "\n",
        "The Apollo program was the United States human spaceflight program carried out by NASA which succeeded in landing the first humans on the Moon. Apollo 11 was the first mission to land on the Moon, commanded by Neil Armstrong and lunar module pilot Buzz Aldrin, with Michael Collins as command module pilot. Armstrong's first step onto the lunar surface occurred on July 20, 1969, and was broadcast on live TV worldwide. The landing required Armstrong to take manual control of the Lunar Module Eagle due to navigational challenges and low fuel.\n",
        "\n",
        "Juno is a NASA space probe orbiting the planet Jupiter. It was launched on August 5, 2011, and entered a polar orbit of Jupiter on July 5, 2016. Juno's mission is to measure Jupiter's composition, gravitational field, magnetic field, and polar magnetosphere to understand how the planet formed. Juno is the second spacecraft to orbit Jupiter, after the Galileo orbiter. It is uniquely powered by large solar arrays instead of RTGs (Radioisotope Thermoelectric Generators), making it the farthest solar-powered mission.\n",
        "\n",
        "A Mars rover is a remote-controlled motor vehicle designed to travel on the surface of Mars. NASA JPL managed several successful rovers including: Sojourner, Spirit, Opportunity, Curiosity, and Perseverance. The search for evidence of habitability and organic carbon on Mars is now a primary NASA objective. Perseverance also carried the Ingenuity helicopter.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "GitdjWeYGrum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f8cf38-6b4e-4e68-f9cd-006bbd6e23e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Loaded 2 documents into the knowledge base.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.Helper Functions for Chunking and Embedding\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Initialize tokenizer for robust, token-aware chunking\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def chunk_text(text, chunk_size=400, overlap=50):\n",
        "    \"\"\"Chunks text based on token count with overlap (Best practice for RAG).\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), chunk_size - overlap):\n",
        "        chunk_tokens = tokens[i:i + chunk_size]\n",
        "        chunk_text = tokenizer.decode(chunk_tokens)\n",
        "        # Basic cleanup\n",
        "        chunk_text = chunk_text.replace(\"\\n\", \" \").strip()\n",
        "        if chunk_text:\n",
        "            chunks.append(chunk_text)\n",
        "    return chunks\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embeddings_batch(texts, model=EMBEDDING_MODEL):\n",
        "    \"\"\"Generates embeddings for a batch of texts using OpenAI, with retries.\"\"\"\n",
        "    # OpenAI expects the input texts to have newlines replaced by spaces\n",
        "    texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
        "    response = client.embeddings.create(input=texts, model=model)\n",
        "    return [item.embedding for item in response.data]\n"
      ],
      "metadata": {
        "id": "5qqwP4AfG0ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nYQq4M8_vP8"
      },
      "outputs": [],
      "source": [
        "#@title Process and Upload Data (High-Fidelity Version)\n",
        "\n",
        "# --- 6.1. Context Library (No Changes) ---\n",
        "print(f\"\\nProcessing and uploading Context Library to namespace: {NAMESPACE_CONTEXT}\")\n",
        "# ... (The existing code for context_blueprints remains the same) ...\n",
        "vectors_context = []\n",
        "for item in tqdm(context_blueprints):\n",
        "    embedding = get_embeddings_batch([item['description']])[0]\n",
        "    vectors_context.append({\n",
        "        \"id\": item['id'],\n",
        "        \"values\": embedding,\n",
        "        \"metadata\": { \"description\": item['description'], \"blueprint_json\": item['blueprint'] }\n",
        "    })\n",
        "if vectors_context:\n",
        "    index.upsert(vectors=vectors_context, namespace=NAMESPACE_CONTEXT)\n",
        "    print(f\"Successfully uploaded {len(vectors_context)} context vectors.\")\n",
        "\n",
        "# --- 6.2. Knowledge Base (UPGRADED FOR HIGH-FIDELITY RAG) ---\n",
        "print(f\"\\nProcessing and uploading Knowledge Base to namespace: {NAMESPACE_KNOWLEDGE}\")\n",
        "batch_size = 100\n",
        "total_vectors_uploaded = 0\n",
        "\n",
        "for doc_name, doc_content in knowledge_base.items():\n",
        "    print(f\"  - Processing document: {doc_name}\")\n",
        "    # Chunk the document content\n",
        "    knowledge_chunks = chunk_text(doc_content)\n",
        "\n",
        "    # Process in batches\n",
        "    for i in tqdm(range(0, len(knowledge_chunks), batch_size), desc=f\"  Uploading {doc_name}\"):\n",
        "        batch_texts = knowledge_chunks[i:i+batch_size]\n",
        "        batch_embeddings = get_embeddings_batch(batch_texts)\n",
        "\n",
        "        batch_vectors = []\n",
        "        for j, embedding in enumerate(batch_embeddings):\n",
        "            chunk_id = f\"{doc_name}_chunk_{total_vectors_uploaded + j}\"\n",
        "\n",
        "            # CRITICAL UPGRADE: Add the 'source' document name to the metadata\n",
        "            batch_vectors.append({\n",
        "                \"id\": chunk_id,\n",
        "                \"values\": embedding,\n",
        "                \"metadata\": {\n",
        "                    \"text\": batch_texts[j],\n",
        "                    \"source\": doc_name  # This is the key to verifiability\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # Upsert the batch\n",
        "        index.upsert(vectors=batch_vectors, namespace=NAMESPACE_KNOWLEDGE)\n",
        "\n",
        "    total_vectors_uploaded += len(knowledge_chunks)\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully uploaded {total_vectors_uploaded} knowledge vectors from {len(knowledge_base)} documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7.Final Verification\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\nIngestion complete. Final Pinecone Index Stats (may take a moment to update):\")\n",
        "time.sleep(15) # Give Pinecone a moment to update stats\n",
        "print(index.describe_index_stats())"
      ],
      "metadata": {
        "id": "6uF27i5EHLjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05fcebd1-ae48-468e-bfc6-6a553fefbce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ingestion complete. Final Pinecone Index Stats (may take a moment to update):\n",
            "{'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {'ContextLibrary': {'vector_count': 3},\n",
            "                'KnowledgeStore': {'vector_count': 2}},\n",
            " 'total_vector_count': 5,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Verify Metadata Ingestion\n",
        "# This step confirms our 'source' metadata was successfully added.\n",
        "import pprint\n",
        "print(\"Querying a sample vector to verify metadata...\")\n",
        "\n",
        "# Get embedding for a sample query\n",
        "query_embedding = get_embeddings_batch([\"What is the Juno mission?\"])[0]\n",
        "\n",
        "# Query Pinecone\n",
        "results = index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=1,\n",
        "    namespace=NAMESPACE_KNOWLEDGE,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "# Print the metadata of the top result\n",
        "if results['matches']:\n",
        "    top_match_metadata = results['matches'][0]['metadata']\n",
        "    print(\"\\n‚úÖ Verification successful! Metadata of top match:\")\n",
        "    pprint.pprint(top_match_metadata)\n",
        "else:\n",
        "    print(\"‚ùå Verification failed. No results found.\")"
      ],
      "metadata": {
        "id": "ZKa3oRN-fEk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21cc022f-6cea-476d-a083-eafeed572b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying a sample vector to verify metadata...\n",
            "\n",
            "‚úÖ Verification successful! Metadata of top match:\n",
            "{'source': 'juno_mission_overview.txt',\n",
            " 'text': \"The Juno mission's primary goal is to understand the origin and \"\n",
            "         'evolution of Jupiter. Underneath its dense cloud cover, Jupiter '\n",
            "         'safeguards secrets to the fundamental processes and conditions that '\n",
            "         'governed our solar system during its formation. As our primary '\n",
            "         'example of a giant planet, Jupiter can also provide critical '\n",
            "         'knowledge for understanding the planetary systems being discovered '\n",
            "         \"around other stars. Juno's specific scientific objectives include: \"\n",
            "         '1. Origin: Determine the abundance of water and constrain the '\n",
            "         \"planet's core mass to decide which theory of the planet's formation \"\n",
            "         'is correct. 2. Atmosphere: Understand the composition, temperature, '\n",
            "         \"cloud motions and other properties of Jupiter's atmosphere. 3. \"\n",
            "         \"Magnetosphere: Map Jupiter's magnetic and gravity fields, revealing \"\n",
            "         \"the planet's deep structure and exploring the polar magnetosphere. \"\n",
            "         'Juno is the first space mission to orbit an outer-planet from pole '\n",
            "         \"to pole, and the first to fly below the planet's hazardous radiation \"\n",
            "         'belts.'}\n"
          ]
        }
      ]
    }
  ]
}