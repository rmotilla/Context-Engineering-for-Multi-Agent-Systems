{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNU4GEGF1wRuzVLWuPi2rdb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipeline - Data Ingestion (Context and Knowledge)\n",
        "\n",
        "Copyright 2025-2026, Denis Rothman\n",
        "\n",
        "This notebook tackles the crucial first step in building a sophisticated RAG pipeline: **data ingestion**. We're essentially setting up the \"brain\" for our agent by feeding it two distinct types of information—the procedural \"how-to\" context and the factual \"what-is\" knowledge. By the end, you'll have a fully prepped Pinecone vector database, ready to power a smarter, context-aware AI.\n",
        "\n",
        "Here’s a quick look at what this notebook does:\n",
        "* **Environment Setup:** Installs all the required Python libraries (`openai`, `pinecone`, `tiktoken`, etc.) and securely configures the necessary API keys.\n",
        "\n",
        "* **Vector Database Prep:** Connects to our Pinecone index, creating it if it doesn't exist and clearing out old data to ensure a clean slate.\n",
        "\n",
        "* **Procedural Context:** Defines and embeds \"Semantic Blueprints\"—our style and structure guides—and uploads them to a dedicated `ContextLibrary` namespace.\n",
        "\n",
        "* **Factual Knowledge:** Chunks raw text into optimized pieces using a tokenizer, creates embeddings for them, and uploads everything to a separate `KnowledgeStore` namespace for factual recall."
      ],
      "metadata": {
        "id": "FTUldUPH_1jM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Installation and Setup"
      ],
      "metadata": {
        "id": "-1bEq01K2Nmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Installation and Setup\n",
        "# -------------------------------------------------------------------------\n",
        "# We install specific versions for stability and reproducibility.\n",
        "# We include tiktoken for token-based chunking and tenacity for robust API calls."
      ],
      "metadata": {
        "id": "_MlRuXOwA7Ai"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm==4.67.1 --upgrade\n",
        "!pip install openai==2.14.0\n",
        "!pip install pinecone==7.0.0 tqdm==4.67.1 tenacity==8.3.0"
      ],
      "metadata": {
        "id": "NlXCn7y6CQ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb65932b-afbf-4377-8ab5-23b742c41e02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting openai==2.14.0\n",
            "  Downloading openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==2.14.0) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==2.14.0) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==2.14.0) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==2.14.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==2.14.0) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.14.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.14.0) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.14.0) (0.4.2)\n",
            "Downloading openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 2.12.0\n",
            "    Uninstalling openai-2.12.0:\n",
            "      Successfully uninstalled openai-2.12.0\n",
            "Successfully installed openai-2.14.0\n",
            "Collecting pinecone==7.0.0\n",
            "  Downloading pinecone-7.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting tenacity==8.3.0\n",
            "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2025.11.12)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone==7.0.0)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone==7.0.0) (1.17.0)\n",
            "Downloading pinecone-7.0.0-py3-none-any.whl (516 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: tenacity, pinecone-plugin-interface, pinecone\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.1.2\n",
            "    Uninstalling tenacity-9.1.2:\n",
            "      Successfully uninstalled tenacity-9.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pinecone-7.0.0 pinecone-plugin-interface-0.0.7 tenacity-8.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for this notebook\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import tiktoken\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "# general imports required in the notebooks of this book\n",
        "import re\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "import copy"
      ],
      "metadata": {
        "id": "ptErFjUn54u0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "EMBEDDING_DIM = 1536 # Dimension for text-embedding-3-small\n",
        "GENERATION_MODEL = \"gpt-5.2\""
      ],
      "metadata": {
        "id": "R9fssMtAwGlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43d0def-e1c8-488b-b590-d4712a3eb386"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Standard way to access secrets securely in Google Colab\n",
        "    from google.colab import userdata\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        raise ValueError(\"API Keys not found in Colab secrets.\")\n",
        "    print(\"API Keys loaded successfully.\")\n",
        "except ImportError:\n",
        "    # Fallback for non-Colab environments (e.g., local Jupyter)\n",
        "    PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        print(\"Warning: API Keys not found. Ensure environment variables are set.\")"
      ],
      "metadata": {
        "id": "d6V_5MOsBeRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93804a8f-73bc-45b4-f2e0-f9e8a60e6a8e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Keys loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Initialize Clients"
      ],
      "metadata": {
        "id": "dxctIvv62hOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Initialize Clients\n",
        "# --- Initialize Clients (assuming this is already done) ---\n",
        "\n",
        "# --- Initialize Pinecone Client ---\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# --- Define Index and Namespaces (assuming this is already done) ---\n",
        "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
        "NAMESPACE_KNOWLEDGE = \"KnowledgeStore\"\n",
        "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
        "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "# Check if index exists\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    print(f\"Index '{INDEX_NAME}' not found. Creating new serverless index...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=EMBEDDING_DIM,\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # Wait for index to be ready\n",
        "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        time.sleep(1)\n",
        "    print(\"Index created successfully. It is new and empty.\")\n",
        "else:\n",
        "    print(f\"Index '{INDEX_NAME}' already exists. Clearing namespaces for a fresh start...\")\n",
        "    index = pc.Index(INDEX_NAME)\n",
        "    namespaces_to_clear = [NAMESPACE_KNOWLEDGE, NAMESPACE_CONTEXT]\n",
        "\n",
        "    for namespace in namespaces_to_clear:\n",
        "        # Check if namespace exists and has vectors before deleting\n",
        "        stats = index.describe_index_stats()\n",
        "        if namespace in stats.namespaces and stats.namespaces[namespace].vector_count > 0:\n",
        "            print(f\"Clearing namespace '{namespace}'...\")\n",
        "            index.delete(delete_all=True, namespace=namespace)\n",
        "\n",
        "            # **CRITICAL FUNCTTION: Wait for deletion to complete**\n",
        "            while True:\n",
        "                stats = index.describe_index_stats()\n",
        "                if namespace not in stats.namespaces or stats.namespaces[namespace].vector_count == 0:\n",
        "                    print(f\"Namespace '{namespace}' cleared successfully.\")\n",
        "                    break\n",
        "                print(f\"Waiting for namespace '{namespace}' to clear...\")\n",
        "                time.sleep(5) # Poll every 5 seconds\n",
        "        else:\n",
        "            print(f\"Namespace '{namespace}' is already empty or does not exist. Skipping.\")\n",
        "\n",
        "# Connect to the index for subsequent operations\n",
        "index = pc.Index(INDEX_NAME)\n"
      ],
      "metadata": {
        "id": "yqAbeOskEjP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4519bd9c-3ebd-4889-c789-2fbf9bd7a82d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'genai-mas-mcp-ch3' already exists. Clearing namespaces for a fresh start...\n",
            "Clearing namespace 'KnowledgeStore'...\n",
            "Namespace 'KnowledgeStore' cleared successfully.\n",
            "Clearing namespace 'ContextLibrary'...\n",
            "Waiting for namespace 'ContextLibrary' to clear...\n",
            "Namespace 'ContextLibrary' cleared successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Data Preparation: The Context Library (Procedural RAG)"
      ],
      "metadata": {
        "id": "hXkeOtzx2Ws-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.Data Preparation: The Context Library (Procedural RAG)\n",
        "# -------------------------------------------------------------------------\n",
        "# We define the Semantic Blueprints derived from Chapter 1.\n",
        "# CRITICAL: We embed the 'description' (the intent), so the Librarian agent\n",
        "# can find the right blueprint based on the desired style. The 'blueprint'\n",
        "# itself is stored as metadata.\n",
        "\n",
        "context_blueprints = [\n",
        "    {\n",
        "        \"id\": \"blueprint_suspense_narrative\",\n",
        "        \"description\": \"A precise Semantic Blueprint designed to generate suspenseful and tense narratives, suitable for children's stories. Focuses on atmosphere, perceived threats, and emotional impact. Ideal for creative writing.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Increase tension and create suspense.\",\n",
        "              \"style_guide\": \"Use short, sharp sentences. Focus on sensory details (sounds, shadows). Maintain a slightly eerie but age-appropriate tone.\",\n",
        "              \"participants\": [\n",
        "                { \"role\": \"Agent\", \"description\": \"The protagonist experiencing the events.\" },\n",
        "                { \"role\": \"Source_of_Threat\", \"description\": \"The underlying danger or mystery.\" }\n",
        "              ],\n",
        "            \"instruction\": \"Rewrite the provided facts into a narrative adhering strictly to the scene_goal and style_guide.\"\n",
        "            })\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"blueprint_technical_explanation\",\n",
        "        \"description\": \"A Semantic Blueprint designed for technical explanation or analysis. This blueprint focuses on clarity, objectivity, and structure. Ideal for breaking down complex processes, explaining mechanisms, or summarizing scientific findings.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Explain the mechanism or findings clearly and concisely.\",\n",
        "              \"style_guide\": \"Maintain an objective and formal tone. Use precise terminology. Prioritize factual accuracy and clarity over narrative flair.\",\n",
        "              \"structure\": [\"Definition\", \"Function/Operation\", \"Key Findings/Impact\"],\n",
        "              \"instruction\": \"Organize the provided facts into the defined structure, adhering to the style_guide.\"\n",
        "            })\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"blueprint_casual_summary\",\n",
        "        \"description\": \"A goal-oriented context for creating a casual, easy-to-read summary. Focuses on brevity and accessibility, explaining concepts simply.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Summarize information quickly and casually.\",\n",
        "              \"style_guide\": \"Use informal language. Keep it brief and engaging. Imagine explaining it to a friend.\",\n",
        "              \"instruction\": \"Summarize the provided facts using the casual style guide.\"\n",
        "            })\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"\\nPrepared {len(context_blueprints)} context blueprints.\")"
      ],
      "metadata": {
        "id": "-XaDNRzlGP1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1eb429a-ca19-463a-f22c-998203a5fdfb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prepared 3 context blueprints.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.Data Preparation: The Knowledge Base (Factual RAG)\n",
        "# -------------------------------------------------------------------------\n",
        "# We use sample data related to space exploration.\n",
        "\n",
        "knowledge_data_raw = \"\"\"\n",
        "Space exploration is the use of astronomy and space technology to explore outer space. The early era of space exploration was driven by a \"Space Race\" between the Soviet Union and the United States. The launch of the Soviet Union's Sputnik 1 in 1957, and the first Moon landing by the American Apollo 11 mission in 1969 are key landmarks.\n",
        "\n",
        "The Apollo program was the United States human spaceflight program carried out by NASA which succeeded in landing the first humans on the Moon. Apollo 11 was the first mission to land on the Moon, commanded by Neil Armstrong and lunar module pilot Buzz Aldrin, with Michael Collins as command module pilot. Armstrong's first step onto the lunar surface occurred on July 20, 1969, and was broadcast on live TV worldwide. The landing required Armstrong to take manual control of the Lunar Module Eagle due to navigational challenges and low fuel.\n",
        "\n",
        "Juno is a NASA space probe orbiting the planet Jupiter. It was launched on August 5, 2011, and entered a polar orbit of Jupiter on July 5, 2016. Juno's mission is to measure Jupiter's composition, gravitational field, magnetic field, and polar magnetosphere to understand how the planet formed. Juno is the second spacecraft to orbit Jupiter, after the Galileo orbiter. It is uniquely powered by large solar arrays instead of RTGs (Radioisotope Thermoelectric Generators), making it the farthest solar-powered mission.\n",
        "\n",
        "A Mars rover is a remote-controlled motor vehicle designed to travel on the surface of Mars. NASA JPL managed several successful rovers including: Sojourner, Spirit, Opportunity, Curiosity, and Perseverance. The search for evidence of habitability and organic carbon on Mars is now a primary NASA objective. Perseverance also carried the Ingenuity helicopter.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "GitdjWeYGrum"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.Helper Functions for Chunking and Embedding\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Initialize tokenizer for robust, token-aware chunking\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def chunk_text(text, chunk_size=400, overlap=50):\n",
        "    \"\"\"Chunks text based on token count with overlap (Best practice for RAG).\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), chunk_size - overlap):\n",
        "        chunk_tokens = tokens[i:i + chunk_size]\n",
        "        chunk_text = tokenizer.decode(chunk_tokens)\n",
        "        # Basic cleanup\n",
        "        chunk_text = chunk_text.replace(\"\\n\", \" \").strip()\n",
        "        if chunk_text:\n",
        "            chunks.append(chunk_text)\n",
        "    return chunks\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embeddings_batch(texts, model=EMBEDDING_MODEL):\n",
        "    \"\"\"Generates embeddings for a batch of texts using OpenAI, with retries.\"\"\"\n",
        "    # OpenAI expects the input texts to have newlines replaced by spaces\n",
        "    texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
        "    response = client.embeddings.create(input=texts, model=model)\n",
        "    return [item.embedding for item in response.data]\n"
      ],
      "metadata": {
        "id": "5qqwP4AfG0ZW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nYQq4M8_vP8"
      },
      "outputs": [],
      "source": [
        "#@title 6.Process and Upload Data\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# --- 6.1. Context Library ---\n",
        "print(f\"\\nProcessing and uploading Context Library to namespace: {NAMESPACE_CONTEXT}\")\n",
        "\n",
        "vectors_context = []\n",
        "for item in tqdm(context_blueprints):\n",
        "    # We embed the DESCRIPTION (the intent)\n",
        "    embedding = get_embeddings_batch([item['description']])[0]\n",
        "    vectors_context.append({\n",
        "        \"id\": item['id'],\n",
        "        \"values\": embedding,\n",
        "        \"metadata\": {\n",
        "            \"description\": item['description'],\n",
        "            # The blueprint itself (JSON string) is stored as metadata\n",
        "            \"blueprint_json\": item['blueprint']\n",
        "        }\n",
        "    })\n",
        "\n",
        "# Upsert data\n",
        "if vectors_context:\n",
        "    index.upsert(vectors=vectors_context, namespace=NAMESPACE_CONTEXT)\n",
        "    print(f\"Successfully uploaded {len(vectors_context)} context vectors.\")\n",
        "\n",
        "# --- 6.2. Knowledge Base ---\n",
        "print(f\"\\nProcessing and uploading Knowledge Base to namespace: {NAMESPACE_KNOWLEDGE}\")\n",
        "\n",
        "# Chunk the knowledge data\n",
        "knowledge_chunks = chunk_text(knowledge_data_raw)\n",
        "print(f\"Created {len(knowledge_chunks)} knowledge chunks.\")\n",
        "\n",
        "vectors_knowledge = []\n",
        "batch_size = 100 # Process in batches\n",
        "\n",
        "for i in tqdm(range(0, len(knowledge_chunks), batch_size)):\n",
        "    batch_texts = knowledge_chunks[i:i+batch_size]\n",
        "    batch_embeddings = get_embeddings_batch(batch_texts)\n",
        "\n",
        "    batch_vectors = []\n",
        "    for j, embedding in enumerate(batch_embeddings):\n",
        "        chunk_id = f\"knowledge_chunk_{i+j}\"\n",
        "        batch_vectors.append({\n",
        "            \"id\": chunk_id,\n",
        "            \"values\": embedding,\n",
        "            \"metadata\": {\n",
        "                \"text\": batch_texts[j]\n",
        "            }\n",
        "        })\n",
        "    # Upsert the batch\n",
        "    index.upsert(vectors=batch_vectors, namespace=NAMESPACE_KNOWLEDGE)\n",
        "\n",
        "print(f\"Successfully uploaded {len(knowledge_chunks)} knowledge vectors.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7.Final Verification\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\nIngestion complete. Final Pinecone Index Stats (may take a moment to update):\")\n",
        "time.sleep(15) # Give Pinecone a moment to update stats\n",
        "print(index.describe_index_stats())"
      ],
      "metadata": {
        "id": "6uF27i5EHLjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9eef65e-93e6-4ed9-9f8d-5060643ac6d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ingestion complete. Final Pinecone Index Stats (may take a moment to update):\n",
            "{'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {'ContextLibrary': {'vector_count': 3},\n",
            "                'KnowledgeStore': {'vector_count': 2}},\n",
            " 'total_vector_count': 5,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ]
    }
  ]
}