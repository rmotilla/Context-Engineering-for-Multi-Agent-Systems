{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMzTVaRXb/KrMQOytwKsqtO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Context-Aware MAS Implementation\n",
        "\n",
        "Copyright 2025, Denis Rothman\n",
        "\n",
        "This notebook implements an educational MAS architecture utilizing RAG via Pinecone with MCP.\n",
        "\n",
        "This notebook implements the execution layer of our Context-Aware system. We'll build a complete Multi-Agent System (MAS) where specialized agents collaborate to fulfill a high-level goal, putting the data we previously ingested into Pinecone to work. The core architecture is designed to cleanly separate procedural instructions (the how) from factual data (the what), enabling highly flexible and controlled content generation.\n",
        "\n",
        "Here's a breakdown of the plan:\n",
        "\n",
        "Agent Definitions: We will code three specialized agents that form the core of the system:\n",
        "\n",
        "The Context Librarian performs procedural RAG to fetch stylistic \"Semantic Blueprints.\"\n",
        "\n",
        "The Researcher uses factual RAG to retrieve and synthesize knowledge on a given topic.\n",
        "\n",
        "The Writer intelligently combines the blueprint and the research to generate the final output.\n",
        "\n",
        "The Orchestrator: This agent acts as the manager. It uses an LLM to analyze a user's goal, breaking it down into distinct intent and topic queries for the other agents.\n",
        "\n",
        "Agent Communication: A simple Message Communication Protocol (MCP) is defined to ensure agents interact in a structured and traceable way.\n",
        "\n",
        "End-to-End Execution: We'll run several examples to demonstrate how the MAS can generate unique outputs for various topics by dynamically retrieving the correct context and knowledge."
      ],
      "metadata": {
        "id": "e6NskRP-IwEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Installation and Setup"
      ],
      "metadata": {
        "id": "-1bEq01K2Nmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Installation and Setup\n",
        "# -------------------------------------------------------------------------\n",
        "# We install specific versions for stability and reproducibility.\n",
        "# We include tiktoken for token-based chunking and tenacity for robust API calls."
      ],
      "metadata": {
        "id": "_MlRuXOwA7Ai"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm==4.67.1 --upgrade\n",
        "!pip install openai==2.8.1\n",
        "!pip install pinecone==7.0.0 tqdm==4.67.1 tenacity==8.3.0"
      ],
      "metadata": {
        "id": "NlXCn7y6CQ3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "EMBEDDING_DIM = 1536 # Dimension for text-embedding-3-small\n",
        "GENERATION_MODEL = \"gpt-5.1\""
      ],
      "metadata": {
        "id": "R9fssMtAwGlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a6a970e-96de-4fbf-a38b-8578442fcce4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for this notebook\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import tiktoken\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "# general imports required in the notebooks of this book\n",
        "import re\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "import copy"
      ],
      "metadata": {
        "id": "ptErFjUn54u0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Standard way to access secrets securely in Google Colab\n",
        "    from google.colab import userdata\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        raise ValueError(\"API Keys not found in Colab secrets.\")\n",
        "    print(\"API Keys loaded successfully.\")\n",
        "except ImportError:\n",
        "    # Fallback for non-Colab environments (e.g., local Jupyter)\n",
        "    PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        print(\"Warning: API Keys not found. Ensure environment variables are set.\")"
      ],
      "metadata": {
        "id": "d6V_5MOsBeRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e13cb3-31f2-43c4-acc5-a2044786cb88"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Keys loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Initialize Clients"
      ],
      "metadata": {
        "id": "dxctIvv62hOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Initialize Clients\n",
        "# --- Initialize Clients (assuming this is already done) ---\n",
        "\n",
        "# --- Initialize Pinecone Client ---\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# --- Define Index and Namespaces (assuming this is already done) ---\n",
        "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
        "NAMESPACE_KNOWLEDGE = \"KnowledgeStore\"\n",
        "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
        "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "# Check if index exists\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    print(f\"Index '{INDEX_NAME}' not found. Creating new serverless index...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=EMBEDDING_DIM, # Make sure EMBEDDING_DIM is defined\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # Wait for index to be ready\n",
        "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        time.sleep(1)\n",
        "    print(\"Index created successfully. It is new and empty.\")\n",
        "else:\n",
        "    # This block runs ONLY if the index already existed.\n",
        "    print(f\"Index '{INDEX_NAME}' already exists.\")\n",
        "\n",
        "    # Connect to the index to perform operations\n",
        "    index = pc.Index(INDEX_NAME)\n",
        "\n",
        "# Connect to the index for subsequent operations\n",
        "index = pc.Index(INDEX_NAME)\n"
      ],
      "metadata": {
        "id": "yqAbeOskEjP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf97f8d-ea31-48f6-9370-d0b71e0afb91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'genai-mas-mcp-ch3' already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Helper Functions (LLM, Embeddings, and MCP)"
      ],
      "metadata": {
        "id": "YpnNWNgcB_jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Helper Functions (LLM, Embeddings, and MCP)\n",
        "# -------------------------------------------------------------------------\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def call_llm(system_prompt, user_prompt, temperature=1, json_mode=False):\n",
        "    \"\"\"A centralized function to handle all LLM interactions with retries.\"\"\"\n",
        "    try:\n",
        "        response_format = {\"type\": \"json_object\"} if json_mode else {\"type\": \"text\"}\n",
        "        response = client.chat.completions.create(\n",
        "            model=GENERATION_MODEL,\n",
        "            response_format=response_format,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling LLM: {e}\")\n",
        "        return f\"LLM Error: {e}\"\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates embeddings for a single text query with retries.\"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input=[text], model=EMBEDDING_MODEL)\n",
        "    return response.data[0].embedding\n",
        "\n",
        "def create_mcp_message(sender, content, metadata=None):\n",
        "    \"\"\"Creates a standardized MCP message (Educational Version).\"\"\"\n",
        "    return {\n",
        "        \"protocol_version\": \"1.1 (RAG-Enhanced)\",\n",
        "        \"sender\": sender,\n",
        "        \"content\": content,\n",
        "        \"metadata\": metadata or {}\n",
        "    }\n",
        "\n",
        "def display_mcp(message, title=\"MCP Message\"):\n",
        "    \"\"\"Helper function to display MCP messages clearly during the trace.\"\"\"\n",
        "    print(f\"\\n--- {title} (Sender: {message['sender']}) ---\")\n",
        "    # Display content snippet or keys if content is complex\n",
        "    if isinstance(message['content'], dict):\n",
        "         print(f\"Content Keys: {list(message['content'].keys())}\")\n",
        "    else:\n",
        "        print(f\"Content: {textwrap.shorten(str(message['content']), width=100)}\")\n",
        "    # Display metadata keys\n",
        "    print(f\"Metadata Keys: {list(message['metadata'].keys())}\")\n",
        "    print(\"-\" * (len(title) + 25))\n",
        "\n",
        "def query_pinecone(query_text, namespace, top_k=1):\n",
        "    \"\"\"Embeds the query text and searches the specified Pinecone namespace.\"\"\"\n",
        "    try:\n",
        "        query_embedding = get_embedding(query_text)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            namespace=namespace,\n",
        "            top_k=top_k,\n",
        "            include_metadata=True\n",
        "        )\n",
        "        return response['matches']\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Pinecone (Namespace: {namespace}): {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"Helper functions and MCP structure defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c5xqzbgK6vS",
        "outputId": "dca39a44-66ea-463e-bbc1-760a7977f374"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions and MCP structure defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.Agent Definitions\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === 4.1. Context Librarian Agent (Procedural RAG) ===\n",
        "def agent_context_librarian(mcp_message):\n",
        "    \"\"\"\n",
        "    Retrieves the appropriate Semantic Blueprint from the Context Library.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Librarian] Activated. Analyzing intent...\")\n",
        "    requested_intent = mcp_message['content']['intent_query']\n",
        "\n",
        "    # Query Pinecone Context Namespace\n",
        "    results = query_pinecone(requested_intent, NAMESPACE_CONTEXT, top_k=1)\n",
        "\n",
        "    if results:\n",
        "        match = results[0]\n",
        "        print(f\"[Librarian] Found blueprint '{match['id']}' (Score: {match['score']:.2f})\")\n",
        "        # Retrieve the blueprint JSON string stored in metadata\n",
        "        blueprint_json = match['metadata']['blueprint_json']\n",
        "        content = {\"blueprint\": blueprint_json}\n",
        "    else:\n",
        "        print(\"[Librarian] No specific blueprint found. Returning default.\")\n",
        "        # Fallback default\n",
        "        content = {\"blueprint\": json.dumps({\"instruction\": \"Generate the content neutrally.\"})}\n",
        "\n",
        "    return create_mcp_message(\"Librarian\", content)\n",
        "\n",
        "# === 4.2. Researcher Agent (Factual RAG) ===\n",
        "def agent_researcher(mcp_message):\n",
        "    \"\"\"\n",
        "    Retrieves and synthesizes factual information from the Knowledge Base.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Researcher] Activated. Investigating topic...\")\n",
        "    topic = mcp_message['content']['topic_query']\n",
        "\n",
        "    # Query Pinecone Knowledge Namespace\n",
        "    results = query_pinecone(topic, NAMESPACE_KNOWLEDGE, top_k=3)\n",
        "\n",
        "    if not results:\n",
        "        print(\"[Researcher] No relevant information found.\")\n",
        "        return create_mcp_message(\"Researcher\", {\"facts\": \"No data found.\"})\n",
        "\n",
        "    # Synthesize the findings (Retrieve-and-Synthesize)\n",
        "    print(f\"[Researcher] Found {len(results)} relevant chunks. Synthesizing...\")\n",
        "    source_texts = [match['metadata']['text'] for match in results]\n",
        "\n",
        "    system_prompt = \"\"\"You are an expert research synthesis AI.\n",
        "    Synthesize the provided source texts into a concise, bullet-pointed summary relevant to the user's topic. Focus strictly on the facts provided in the sources. Do not add outside information.\"\"\"\n",
        "\n",
        "    user_prompt = f\"Topic: {topic}\\n\\nSources:\\n\" + \"\\n\\n---\\n\\n\".join(source_texts)\n",
        "\n",
        "    findings = call_llm(system_prompt, user_prompt)\n",
        "\n",
        "    return create_mcp_message(\"Researcher\", {\"facts\": findings})\n",
        "\n",
        "# === 4.3. Writer Agent (Generation) ===\n",
        "def agent_writer(mcp_message):\n",
        "    \"\"\"\n",
        "    Combines the factual research with the semantic blueprint to generate the final output.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Writer] Activated. Applying blueprint to facts...\")\n",
        "\n",
        "    facts = mcp_message['content']['facts']\n",
        "    # The blueprint is passed as a JSON string\n",
        "    blueprint_json_string = mcp_message['content']['blueprint']\n",
        "\n",
        "    # The Writer's System Prompt incorporates the dynamically retrieved blueprint\n",
        "    system_prompt = f\"\"\"You are an expert content generation AI.\n",
        "    Your task is to generate content based on the provided RESEARCH FINDINGS.\n",
        "    Crucially, you MUST structure, style, and constrain your output according to the rules defined in the SEMANTIC BLUEPRINT provided below.\n",
        "\n",
        "    --- SEMANTIC BLUEPRINT (JSON) ---\n",
        "    {blueprint_json_string}\n",
        "    --- END SEMANTIC BLUEPRINT ---\n",
        "\n",
        "    Adhere strictly to the blueprint's instructions, style guides, and goals. The blueprint defines HOW you write; the research defines WHAT you write about.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    --- RESEARCH FINDINGS ---\n",
        "    {facts}\n",
        "    --- END RESEARCH FINDINGS ---\n",
        "\n",
        "    Generate the content now.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the final content (slightly higher temperature for potential creativity)\n",
        "    final_output = call_llm(system_prompt, user_prompt)\n",
        "\n",
        "    return create_mcp_message(\"Writer\", {\"output\": final_output})"
      ],
      "metadata": {
        "id": "MGu0zBuGLKnI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.The Orchestrator\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "def orchestrator(high_level_goal):\n",
        "    \"\"\"\n",
        "    Manages the workflow of the Context-Aware MAS.\n",
        "    Analyzes the goal, retrieves context and facts, and coordinates generation.\n",
        "    \"\"\"\n",
        "    print(f\"=== [Orchestrator] Starting New Task ===\")\n",
        "    print(f\"Goal: {high_level_goal}\")\n",
        "\n",
        "    # Step 0: Analyze Goal (Determine Intent and Topic)\n",
        "    # We use the LLM to separate the desired style (intent) from the subject matter (topic).\n",
        "    print(\"\\n[Orchestrator] Analyzing Goal...\")\n",
        "    analysis_system_prompt = \"\"\"You are an expert goal analyst. Analyze the user's high-level goal and extract two components:\n",
        "    1. 'intent_query': A descriptive phrase summarizing the desired style, tone, or format, optimized for searching a context library (e.g., \"suspenseful narrative blueprint\", \"objective technical explanation structure\").\n",
        "    2. 'topic_query': A concise phrase summarizing the factual subject matter required (e.g., \"Juno mission objectives and power\", \"Apollo 11 landing details\").\n",
        "\n",
        "    Respond ONLY with a JSON object containing these two keys.\"\"\"\n",
        "\n",
        "    # We request JSON mode for reliable parsing\n",
        "    analysis_result = call_llm(analysis_system_prompt, high_level_goal, json_mode=True)\n",
        "\n",
        "    try:\n",
        "        analysis = json.loads(analysis_result)\n",
        "        intent_query = analysis['intent_query']\n",
        "        topic_query = analysis['topic_query']\n",
        "    except (json.JSONDecodeError, KeyError):\n",
        "        print(f\"[Orchestrator] Error: Could not parse analysis JSON. Raw Analysis: {analysis_result}. Aborting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Orchestrator: Intent Query: '{intent_query}'\")\n",
        "    print(f\"Orchestrator: Topic Query: '{topic_query}'\")\n",
        "\n",
        "\n",
        "    # Step 1: Get the Context Blueprint (Procedural RAG)\n",
        "    mcp_to_librarian = create_mcp_message(\n",
        "        sender=\"Orchestrator\",\n",
        "        content={\"intent_query\": intent_query}\n",
        "    )\n",
        "    # display_mcp(mcp_to_librarian, \"Orchestrator -> Librarian\")\n",
        "    mcp_from_librarian = agent_context_librarian(mcp_to_librarian)\n",
        "    display_mcp(mcp_from_librarian, \"Librarian -> Orchestrator\")\n",
        "\n",
        "    context_blueprint = mcp_from_librarian['content'].get('blueprint')\n",
        "    if not context_blueprint: return\n",
        "\n",
        "    # Step 2: Get the Factual Knowledge (Factual RAG)\n",
        "    mcp_to_researcher = create_mcp_message(\n",
        "        sender=\"Orchestrator\",\n",
        "        content={\"topic_query\": topic_query}\n",
        "    )\n",
        "    # display_mcp(mcp_to_researcher, \"Orchestrator -> Researcher\")\n",
        "    mcp_from_researcher = agent_researcher(mcp_to_researcher)\n",
        "    display_mcp(mcp_from_researcher, \"Researcher -> Orchestrator\")\n",
        "\n",
        "    research_findings = mcp_from_researcher['content'].get('facts')\n",
        "    if not research_findings: return\n",
        "\n",
        "    # Step 3: Generate the Final Output\n",
        "    # Combine the outputs for the Writer Agent\n",
        "    writer_task = {\n",
        "        \"blueprint\": context_blueprint,\n",
        "        \"facts\": research_findings\n",
        "    }\n",
        "\n",
        "    mcp_to_writer = create_mcp_message(\n",
        "        sender=\"Orchestrator\",\n",
        "        content=writer_task\n",
        "    )\n",
        "    # display_mcp(mcp_to_writer, \"Orchestrator -> Writer\")\n",
        "    mcp_from_writer = agent_writer(mcp_to_writer)\n",
        "    display_mcp(mcp_from_writer, \"Writer -> Orchestrator\")\n",
        "\n",
        "    final_result = mcp_from_writer['content'].get('output')\n",
        "\n",
        "    print(\"\\n=== [Orchestrator] Task Complete ===\")\n",
        "    return final_result"
      ],
      "metadata": {
        "id": "kPEcJnBwLUQc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  6.Running examples\n"
      ],
      "metadata": {
        "id": "NcptoMmoLwKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example 1: Requesting a specific style (Suspense) for a topic (Apollo 11)\n",
        "print(\"********  1: SUSPENSEFUL NARRATIVE **********\")\n",
        "goal_1 = \"Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\"\n",
        "result_1 = orchestrator(goal_1)\n",
        "\n",
        "print(\"\\n******** FINAL OUTPUT 1 **********\\n\")\n",
        "print(result_1)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VclgEP1qLp5l",
        "outputId": "98ae6e2d-9c40-4d61-c62a-440b48983d82"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********  1: SUSPENSEFUL NARRATIVE **********\n",
            "=== [Orchestrator] Starting New Task ===\n",
            "Goal: Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\n",
            "\n",
            "[Orchestrator] Analyzing Goal...\n",
            "Orchestrator: Intent Query: 'short suspenseful children’s story scene structure'\n",
            "Orchestrator: Topic Query: 'Apollo 11 moon landing and associated dangers'\n",
            "\n",
            "[Librarian] Activated. Analyzing intent...\n",
            "[Librarian] Found blueprint 'blueprint_suspense_narrative' (Score: 0.57)\n",
            "\n",
            "--- Librarian -> Orchestrator (Sender: Librarian) ---\n",
            "Content Keys: ['blueprint']\n",
            "Metadata Keys: []\n",
            "--------------------------------------------------\n",
            "\n",
            "[Researcher] Activated. Investigating topic...\n",
            "[Researcher] Found 2 relevant chunks. Synthesizing...\n",
            "\n",
            "--- Researcher -> Orchestrator (Sender: Researcher) ---\n",
            "Content Keys: ['facts']\n",
            "Metadata Keys: []\n",
            "---------------------------------------------------\n",
            "\n",
            "[Writer] Activated. Applying blueprint to facts...\n",
            "\n",
            "--- Writer -> Orchestrator (Sender: Writer) ---\n",
            "Content Keys: ['output']\n",
            "Metadata Keys: []\n",
            "-----------------------------------------------\n",
            "\n",
            "=== [Orchestrator] Task Complete ===\n",
            "\n",
            "******** FINAL OUTPUT 1 **********\n",
            "\n",
            "The world goes quiet first.\n",
            "\n",
            "No engines. No crowd. Just a soft hiss in the headset and the thin, steady beep of a machine watching the Agent’s heartbeat.\n",
            "\n",
            "Outside is black. Not night-black. Deeper. The kind that swallows light and keeps it.\n",
            "\n",
            "The Agent floats, strapped in, eyes fixed on the tiny window.\n",
            "\n",
            "The Source_of_Threat waits below. A dead gray world. No air. No sound. No second chances.\n",
            "\n",
            "“Program alarm,” a voice crackles. Distant. Tinny. From far, far away.\n",
            "\n",
            "Numbers flash. Warnings blink. The Lunar Module shudders as it falls.\n",
            "\n",
            "Automatic systems chatter and strain. They see rocks, craters, uneven ground. The machine wants to land where it shouldn’t. Where it can’t.\n",
            "\n",
            "The Agent feels it in his hands. In the fragile metal shell around him. In the tightness of his breath.\n",
            "\n",
            "The computer is not enough.\n",
            "\n",
            "He switches to manual.\n",
            "\n",
            "Now his hands are on the Source_of_Threat. Guiding. Fighting. The descent engine roars below his feet, a low, growling thunder through thin walls.\n",
            "\n",
            "Dust waits down there, fine as ash. A wrong move and it will rise like a blinding storm, hiding jagged stones, hidden slopes. A wrong move and the Module tips. Crumples. Silence, forever.\n",
            "\n",
            "He checks the gauges.\n",
            "\n",
            "Fuel.\n",
            "\n",
            "The number is falling fast.\n",
            "\n",
            "Each second chews it away. Each correction. Each tiny nudge to dodge a crater, to slide past a field of boulders that loom like frozen waves of stone.\n",
            "\n",
            "“Thirty seconds.”\n",
            "\n",
            "The voice from Earth is calm. Too calm. The words are cold metal.\n",
            "\n",
            "Thirty seconds of fuel until the engine dies.\n",
            "\n",
            "Thirty seconds between landing and falling.\n",
            "\n",
            "The Agent’s heartbeat thumps loud in the helmet. Louder than the engine. Louder than the voices.\n",
            "\n",
            "Shadows slide under the Module, long and sharp. Crater rims stretch like black claws across the gray.\n",
            "\n",
            "He can see the surface now. Rocky. Uneven. Not here. Not yet.\n",
            "\n",
            "He eases the Module forward. Sideways. Searching for something flat. Something safe enough to live on.\n",
            "\n",
            "Fuel ticks down.\n",
            "\n",
            "“Forward... forward...” he mutters, voice tight, more to himself than anyone else.\n",
            "\n",
            "The Module sinks.\n",
            "\n",
            "Dust begins to stir below, kicked up by the blast. It swirls in slow, ghostly curls. No wind to move it. Just the engine’s invisible hand.\n",
            "\n",
            "The Agent grips the controls. No drifting. No sudden tilt. If the legs catch wrong, if the Module slips—\n",
            "\n",
            "No.\n",
            "\n",
            "He keeps it steady. Eyes never leaving the patch of ground just ahead. A brighter patch. Smoother. Maybe.\n",
            "\n",
            "“Contact light.”\n",
            "\n",
            "The word cuts through the tension like a spark.\n",
            "\n",
            "A small blue light glows. The metal leg has touched the Moon.\n",
            "\n",
            "But they are not safe yet. The Agent eases the engine down, careful. Too fast and they’ll slam. Too slow and they’ll hover, eating the last drops of fuel.\n",
            "\n",
            "A soft thud. A gentle lean.\n",
            "\n",
            "The shaking stops.\n",
            "\n",
            "The engine falls silent.\n",
            "\n",
            "For a second, no one speaks.\n",
            "\n",
            "The threat doesn’t vanish. It simply waits outside, thin and cold and airless. A wrong seal. A torn line. A slip of the hand on that powdery ground.\n",
            "\n",
            "But for now, the Module stands.\n",
            "\n",
            "Miles above, the third man circles alone. Michael Collins, the command module pilot. The safety net that cannot catch them if they fall. If they fail to rise, he will leave them here and go home alone.\n",
            "\n",
            "The Agent listens. The world holds its breath.\n",
            "\n",
            "Then, the voice from home:\n",
            "\n",
            "“Houston, Tranquility Base here. The Eagle has landed.”\n",
            "\n",
            "The words echo through the tiny cabin. Through distant speakers across a distant world.\n",
            "\n",
            "The danger has not passed. But something has changed.\n",
            "\n",
            "Later, the hatch opens.\n",
            "\n",
            "The Agent moves carefully, every sound magnified. Suit joints creak. Fans whir softly. His own breath rasps loud in his ears.\n",
            "\n",
            "He leans forward. Ladder rungs tap under his boots. One misstep, one stumble, and there is no one to catch him.\n",
            "\n",
            "At the bottom, he pauses. Shadows reach up around his feet, sharp and black as ink. The Moon stretches out, silent and watching.\n",
            "\n",
            "He steps down.\n",
            "\n",
            "One foot presses into alien dust.\n",
            "\n",
            "The Source_of_Threat is under him now, all around him, in every empty inch of space.\n",
            "\n",
            "The world is listening, but it cannot help him. Not here.\n",
            "\n",
            "He stands alone in the quiet, heart pounding, on a place where no human has ever stood, knowing how close he came to never standing at all.\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example 2: Requesting a different style (Technical) for another topic (Juno)\n",
        "print(\"******** 2: TECHNICAL EXPLANATION **********\")\n",
        "goal_2 = \"Provide a clear technical explanation of the Juno mission's objectives and how it is powered.\"\n",
        "result_2 = orchestrator(goal_2)\n",
        "\n",
        "print(\"\\n******** FINAL OUTPUT 2 **********\\n\")\n",
        "print(result_2)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij6p9Ph5MCiC",
        "outputId": "7cd9926f-e281-46ac-b171-b318eca2f5c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******** 2: TECHNICAL EXPLANATION **********\n",
            "=== [Orchestrator] Starting New Task ===\n",
            "Goal: Provide a clear technical explanation of the Juno mission's objectives and how it is powered.\n",
            "\n",
            "[Orchestrator] Analyzing Goal...\n",
            "Orchestrator: Intent Query: 'clear technical explanation structure'\n",
            "Orchestrator: Topic Query: 'Juno mission objectives and power source details'\n",
            "\n",
            "[Librarian] Activated. Analyzing intent...\n",
            "[Librarian] Found blueprint 'blueprint_technical_explanation' (Score: 0.49)\n",
            "\n",
            "--- Librarian -> Orchestrator (Sender: Librarian) ---\n",
            "Content Keys: ['blueprint']\n",
            "Metadata Keys: []\n",
            "--------------------------------------------------\n",
            "\n",
            "[Researcher] Activated. Investigating topic...\n",
            "[Researcher] Found 2 relevant chunks. Synthesizing...\n",
            "\n",
            "--- Researcher -> Orchestrator (Sender: Researcher) ---\n",
            "Content Keys: ['facts']\n",
            "Metadata Keys: []\n",
            "---------------------------------------------------\n",
            "\n",
            "[Writer] Activated. Applying blueprint to facts...\n",
            "\n",
            "--- Writer -> Orchestrator (Sender: Writer) ---\n",
            "Content Keys: ['output']\n",
            "Metadata Keys: []\n",
            "-----------------------------------------------\n",
            "\n",
            "=== [Orchestrator] Task Complete ===\n",
            "\n",
            "******** FINAL OUTPUT 2 **********\n",
            "\n",
            "**Definition**  \n",
            "Juno is a NASA space probe specifically designed to study the planet Jupiter from orbit. It is the second spacecraft to orbit Jupiter, following the Galileo orbiter. Juno was launched on August 5, 2011, and entered a polar orbit around Jupiter on July 5, 2016.\n",
            "\n",
            "**Function/Operation**  \n",
            "Juno operates in a highly elliptical polar orbit around Jupiter, enabling repeated passes over the planet’s poles and through different regions of its surrounding environment. Its primary operational tasks are to measure Jupiter’s composition, gravitational field, magnetic field, and polar magnetosphere. These measurements are acquired using onboard scientific instruments optimized for precise field and particle detection and for probing the planet’s deep interior. Uniquely, Juno is powered by large solar arrays rather than radioisotope thermoelectric generators, making it capable of sustained operation at Jupiter’s distance from the Sun as the farthest solar-powered mission to date.\n",
            "\n",
            "**Key Findings/Impact**  \n",
            "By characterizing Jupiter’s composition and mapping its gravitational and magnetic fields, Juno provides critical data on the planet’s internal structure and dynamics. The measurements of the polar magnetosphere yield insights into Jupiter’s powerful magnetic environment and its interaction with the solar wind. Collectively, these observations are used to constrain models of Jupiter’s formation and evolution, thereby informing broader theories of giant planet formation and the early history of the Solar System.\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Example 3: Requesting a casual style\n",
        "print(\"******** 3: CASUAL SUMMARY **********\")\n",
        "goal_3 = \"Give me a quick, casual summary of what Mars rovers do.\"\n",
        "result_3 = orchestrator(goal_3)\n",
        "\n",
        "print(\"\\n******** FINAL OUTPUT 3 **********\\n\")\n",
        "print(result_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkRChqNiMJOx",
        "outputId": "6d08633b-cdc6-413d-a510-af75c0e655bf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******** 3: CASUAL SUMMARY **********\n",
            "=== [Orchestrator] Starting New Task ===\n",
            "Goal: Give me a quick, casual summary of what Mars rovers do.\n",
            "\n",
            "[Orchestrator] Analyzing Goal...\n",
            "Orchestrator: Intent Query: 'quick casual explanatory summary'\n",
            "Orchestrator: Topic Query: 'roles and activities of Mars rovers'\n",
            "\n",
            "[Librarian] Activated. Analyzing intent...\n",
            "[Librarian] Found blueprint 'blueprint_casual_summary' (Score: 0.64)\n",
            "\n",
            "--- Librarian -> Orchestrator (Sender: Librarian) ---\n",
            "Content Keys: ['blueprint']\n",
            "Metadata Keys: []\n",
            "--------------------------------------------------\n",
            "\n",
            "[Researcher] Activated. Investigating topic...\n",
            "[Researcher] Found 2 relevant chunks. Synthesizing...\n",
            "\n",
            "--- Researcher -> Orchestrator (Sender: Researcher) ---\n",
            "Content Keys: ['facts']\n",
            "Metadata Keys: []\n",
            "---------------------------------------------------\n",
            "\n",
            "[Writer] Activated. Applying blueprint to facts...\n",
            "\n",
            "--- Writer -> Orchestrator (Sender: Writer) ---\n",
            "Content Keys: ['output']\n",
            "Metadata Keys: []\n",
            "-----------------------------------------------\n",
            "\n",
            "=== [Orchestrator] Task Complete ===\n",
            "\n",
            "******** FINAL OUTPUT 3 **********\n",
            "\n",
            "Mars rovers are basically robot cars we drive around Mars from Earth to check out what it’s like over there.\n",
            "\n",
            "NASA (through JPL) has sent a whole lineup: Sojourner, Spirit, Opportunity, Curiosity, and now Perseverance. Each one’s been like the next upgrade in the series.\n",
            "\n",
            "Their main jobs:\n",
            "- See if Mars could ever have supported life (habitability).\n",
            "- Look for organic carbon (the kind of stuff life is made of).\n",
            "- Explore the surface to back up those science goals.\n",
            "\n",
            "Perseverance keeps that same mission but with extra swag: it brought along the Ingenuity helicopter, so now we’re not just rolling on the ground—we’re flying around too.\n"
          ]
        }
      ]
    }
  ]
}