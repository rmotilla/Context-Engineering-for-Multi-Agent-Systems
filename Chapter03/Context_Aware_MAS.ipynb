{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMzTVaRXb/KrMQOytwKsqtO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Context-Aware MAS Implementation\n",
        "\n",
        "Copyright 2025, Denis Rothman\n",
        "\n",
        "This notebook implements an educational MAS architecture utilizing RAG via Pinecone with MCP.\n",
        "\n",
        "This notebook implements the execution layer of our Context-Aware system. We'll build a complete Multi-Agent System (MAS) where specialized agents collaborate to fulfill a high-level goal, putting the data we previously ingested into Pinecone to work. The core architecture is designed to cleanly separate procedural instructions (the how) from factual data (the what), enabling highly flexible and controlled content generation.\n",
        "\n",
        "Here's a breakdown of the plan:\n",
        "\n",
        "Agent Definitions: We will code three specialized agents that form the core of the system:\n",
        "\n",
        "The Context Librarian performs procedural RAG to fetch stylistic \"Semantic Blueprints.\"\n",
        "\n",
        "The Researcher uses factual RAG to retrieve and synthesize knowledge on a given topic.\n",
        "\n",
        "The Writer intelligently combines the blueprint and the research to generate the final output.\n",
        "\n",
        "The Orchestrator: This agent acts as the manager. It uses an LLM to analyze a user's goal, breaking it down into distinct intent and topic queries for the other agents.\n",
        "\n",
        "Agent Communication: A simple Message Communication Protocol (MCP) is defined to ensure agents interact in a structured and traceable way.\n",
        "\n",
        "End-to-End Execution: We'll run several examples to demonstrate how the MAS can generate unique outputs for various topics by dynamically retrieving the correct context and knowledge."
      ],
      "metadata": {
        "id": "e6NskRP-IwEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Installation and Setup"
      ],
      "metadata": {
        "id": "-1bEq01K2Nmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Installation and Setup\n",
        "# -------------------------------------------------------------------------\n",
        "# We install specific versions for stability and reproducibility.\n",
        "# We include tiktoken for token-based chunking and tenacity for robust API calls."
      ],
      "metadata": {
        "id": "_MlRuXOwA7Ai"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm==4.67.1 --upgrade\n",
        "!pip install openai==2.8.1\n",
        "!pip install pinecone==7.0.0 tqdm==4.67.1 tenacity==8.3.0"
      ],
      "metadata": {
        "id": "NlXCn7y6CQ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acf9c91c-6846-48ec-8d75-6330a2979071"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting openai==2.8.1\n",
            "  Downloading openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==2.8.1) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==2.8.1) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==2.8.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==2.8.1) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.8.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.8.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.8.1) (0.4.2)\n",
            "Downloading openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.109.1\n",
            "    Uninstalling openai-1.109.1:\n",
            "      Successfully uninstalled openai-1.109.1\n",
            "Successfully installed openai-2.8.1\n",
            "Collecting pinecone==7.0.0\n",
            "  Downloading pinecone-7.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting tenacity==8.3.0\n",
            "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2025.10.5)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone==7.0.0)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone==7.0.0) (1.17.0)\n",
            "Downloading pinecone-7.0.0-py3-none-any.whl (516 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: tenacity, pinecone-plugin-interface, pinecone\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 8.5.0\n",
            "    Uninstalling tenacity-8.5.0:\n",
            "      Successfully uninstalled tenacity-8.5.0\n",
            "Successfully installed pinecone-7.0.0 pinecone-plugin-interface-0.0.7 tenacity-8.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "EMBEDDING_DIM = 1536 # Dimension for text-embedding-3-small\n",
        "GENERATION_MODEL = \"gpt-5.1\""
      ],
      "metadata": {
        "id": "R9fssMtAwGlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1c894f-fd73-4ad6-c9ce-ced7fcbfb586"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for this notebook\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import tiktoken\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "# general imports required in the notebooks of this book\n",
        "import re\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "import copy"
      ],
      "metadata": {
        "id": "ptErFjUn54u0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Standard way to access secrets securely in Google Colab\n",
        "    from google.colab import userdata\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        raise ValueError(\"API Keys not found in Colab secrets.\")\n",
        "    print(\"API Keys loaded successfully.\")\n",
        "except ImportError:\n",
        "    # Fallback for non-Colab environments (e.g., local Jupyter)\n",
        "    PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        print(\"Warning: API Keys not found. Ensure environment variables are set.\")"
      ],
      "metadata": {
        "id": "d6V_5MOsBeRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba8877e-ae0f-41fa-e4c7-9573293882fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Keys loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Initialize Clients"
      ],
      "metadata": {
        "id": "dxctIvv62hOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Initialize Clients\n",
        "# --- Initialize Clients (assuming this is already done) ---\n",
        "\n",
        "# --- Initialize Pinecone Client ---\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# --- Define Index and Namespaces (assuming this is already done) ---\n",
        "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
        "NAMESPACE_KNOWLEDGE = \"KnowledgeStore\"\n",
        "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
        "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "# Check if index exists\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    print(f\"Index '{INDEX_NAME}' not found. Creating new serverless index...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=EMBEDDING_DIM, # Make sure EMBEDDING_DIM is defined\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # Wait for index to be ready\n",
        "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        time.sleep(1)\n",
        "    print(\"Index created successfully. It is new and empty.\")\n",
        "else:\n",
        "    # This block runs ONLY if the index already existed.\n",
        "    print(f\"Index '{INDEX_NAME}' already exists.\")\n",
        "\n",
        "    # Connect to the index to perform operations\n",
        "    index = pc.Index(INDEX_NAME)\n",
        "\n",
        "# Connect to the index for subsequent operations\n",
        "index = pc.Index(INDEX_NAME)\n"
      ],
      "metadata": {
        "id": "yqAbeOskEjP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae537fe2-6609-4094-b99f-72692f35705a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'genai-mas-mcp-ch3' already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Helper Functions (LLM, Embeddings, and MCP)"
      ],
      "metadata": {
        "id": "YpnNWNgcB_jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Helper Functions (LLM, Embeddings, and MCP)\n",
        "# -------------------------------------------------------------------------\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def call_llm(system_prompt, user_prompt, temperature=1, json_mode=False):\n",
        "    \"\"\"A centralized function to handle all LLM interactions with retries.\"\"\"\n",
        "    try:\n",
        "        response_format = {\"type\": \"json_object\"} if json_mode else {\"type\": \"text\"}\n",
        "        response = client.chat.completions.create(\n",
        "            model=GENERATION_MODEL,\n",
        "            response_format=response_format,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling LLM: {e}\")\n",
        "        return f\"LLM Error: {e}\"\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates embeddings for a single text query with retries.\"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input=[text], model=EMBEDDING_MODEL)\n",
        "    return response.data[0].embedding\n",
        "\n",
        "def create_mcp_message(sender, content, metadata=None):\n",
        "    \"\"\"Creates a standardized MCP message (Educational Version).\"\"\"\n",
        "    return {\n",
        "        \"protocol_version\": \"1.1 (RAG-Enhanced)\",\n",
        "        \"sender\": sender,\n",
        "        \"content\": content,\n",
        "        \"metadata\": metadata or {}\n",
        "    }\n",
        "\n",
        "def display_mcp(message, title=\"MCP Message\"):\n",
        "    \"\"\"Helper function to display MCP messages clearly during the trace.\"\"\"\n",
        "    print(f\"\\n--- {title} (Sender: {message['sender']}) ---\")\n",
        "    # Display content snippet or keys if content is complex\n",
        "    if isinstance(message['content'], dict):\n",
        "         print(f\"Content Keys: {list(message['content'].keys())}\")\n",
        "    else:\n",
        "        print(f\"Content: {textwrap.shorten(str(message['content']), width=100)}\")\n",
        "    # Display metadata keys\n",
        "    print(f\"Metadata Keys: {list(message['metadata'].keys())}\")\n",
        "    print(\"-\" * (len(title) + 25))\n",
        "\n",
        "def query_pinecone(query_text, namespace, top_k=1):\n",
        "    \"\"\"Embeds the query text and searches the specified Pinecone namespace.\"\"\"\n",
        "    try:\n",
        "        query_embedding = get_embedding(query_text)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            namespace=namespace,\n",
        "            top_k=top_k,\n",
        "            include_metadata=True\n",
        "        )\n",
        "        return response['matches']\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Pinecone (Namespace: {namespace}): {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"Helper functions and MCP structure defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c5xqzbgK6vS",
        "outputId": "9eead82d-205d-4924-f4eb-fdeaee4cb28c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions and MCP structure defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.Agent Definitions\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === 4.1. Context Librarian Agent (Procedural RAG) ===\n",
        "def agent_context_librarian(mcp_message):\n",
        "    \"\"\"\n",
        "    Retrieves the appropriate Semantic Blueprint from the Context Library.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Librarian] Activated. Analyzing intent...\")\n",
        "    requested_intent = mcp_message['content']['intent_query']\n",
        "\n",
        "    # Query Pinecone Context Namespace\n",
        "    results = query_pinecone(requested_intent, NAMESPACE_CONTEXT, top_k=1)\n",
        "\n",
        "    if results:\n",
        "        match = results[0]\n",
        "        print(f\"[Librarian] Found blueprint '{match['id']}' (Score: {match['score']:.2f})\")\n",
        "        # Retrieve the blueprint JSON string stored in metadata\n",
        "        blueprint_json = match['metadata']['blueprint_json']\n",
        "        content = {\"blueprint\": blueprint_json}\n",
        "    else:\n",
        "        print(\"[Librarian] No specific blueprint found. Returning default.\")\n",
        "        # Fallback default\n",
        "        content = {\"blueprint\": json.dumps({\"instruction\": \"Generate the content neutrally.\"})}\n",
        "\n",
        "    return create_mcp_message(\"Librarian\", content)\n",
        "\n",
        "# === 4.2. Researcher Agent (Factual RAG) ===\n",
        "def agent_researcher(mcp_message):\n",
        "    \"\"\"\n",
        "    Retrieves and synthesizes factual information from the Knowledge Base.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Researcher] Activated. Investigating topic...\")\n",
        "    topic = mcp_message['content']['topic_query']\n",
        "\n",
        "    # Query Pinecone Knowledge Namespace\n",
        "    results = query_pinecone(topic, NAMESPACE_KNOWLEDGE, top_k=3)\n",
        "\n",
        "    if not results:\n",
        "        print(\"[Researcher] No relevant information found.\")\n",
        "        return create_mcp_message(\"Researcher\", {\"facts\": \"No data found.\"})\n",
        "\n",
        "    # Synthesize the findings (Retrieve-and-Synthesize)\n",
        "    print(f\"[Researcher] Found {len(results)} relevant chunks. Synthesizing...\")\n",
        "    source_texts = [match['metadata']['text'] for match in results]\n",
        "\n",
        "    system_prompt = \"\"\"You are an expert research synthesis AI.\n",
        "    Synthesize the provided source texts into a concise, bullet-pointed summary relevant to the user's topic. Focus strictly on the facts provided in the sources. Do not add outside information.\"\"\"\n",
        "\n",
        "    user_prompt = f\"Topic: {topic}\\n\\nSources:\\n\" + \"\\n\\n---\\n\\n\".join(source_texts)\n",
        "\n",
        "    findings = call_llm(system_prompt, user_prompt)\n",
        "\n",
        "    return create_mcp_message(\"Researcher\", {\"facts\": findings})\n",
        "\n",
        "# === 4.3. Writer Agent (Generation) ===\n",
        "def agent_writer(mcp_message):\n",
        "    \"\"\"\n",
        "    Combines the factual research with the semantic blueprint to generate the final output.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Writer] Activated. Applying blueprint to facts...\")\n",
        "\n",
        "    facts = mcp_message['content']['facts']\n",
        "    # The blueprint is passed as a JSON string\n",
        "    blueprint_json_string = mcp_message['content']['blueprint']\n",
        "\n",
        "    # The Writer's System Prompt incorporates the dynamically retrieved blueprint\n",
        "    system_prompt = f\"\"\"You are an expert content generation AI.\n",
        "    Your task is to generate content based on the provided RESEARCH FINDINGS.\n",
        "    Crucially, you MUST structure, style, and constrain your output according to the rules defined in the SEMANTIC BLUEPRINT provided below.\n",
        "\n",
        "    --- SEMANTIC BLUEPRINT (JSON) ---\n",
        "    {blueprint_json_string}\n",
        "    --- END SEMANTIC BLUEPRINT ---\n",
        "\n",
        "    Adhere strictly to the blueprint's instructions, style guides, and goals. The blueprint defines HOW you write; the research defines WHAT you write about.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    --- RESEARCH FINDINGS ---\n",
        "    {facts}\n",
        "    --- END RESEARCH FINDINGS ---\n",
        "\n",
        "    Generate the content now.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the final content (slightly higher temperature for potential creativity)\n",
        "    final_output = call_llm(system_prompt, user_prompt)\n",
        "\n",
        "    return create_mcp_message(\"Writer\", {\"output\": final_output})"
      ],
      "metadata": {
        "id": "MGu0zBuGLKnI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.The Orchestrator\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "def orchestrator(high_level_goal):\n",
        "    \"\"\"\n",
        "    Manages the workflow of the Context-Aware MAS.\n",
        "    Analyzes the goal, retrieves context and facts, and coordinates generation.\n",
        "    \"\"\"\n",
        "    print(f\"=== [Orchestrator] Starting New Task ===\")\n",
        "    print(f\"Goal: {high_level_goal}\")\n",
        "\n",
        "    # Step 0: Analyze Goal (Determine Intent and Topic)\n",
        "    # We use the LLM to separate the desired style (intent) from the subject matter (topic).\n",
        "    print(\"\\n[Orchestrator] Analyzing Goal...\")\n",
        "    analysis_system_prompt = \"\"\"You are an expert goal analyst. Analyze the user's high-level goal and extract two components:\n",
        "    1. 'intent_query': A descriptive phrase summarizing the desired style, tone, or format, optimized for searching a context library (e.g., \"suspenseful narrative blueprint\", \"objective technical explanation structure\").\n",
        "    2. 'topic_query': A concise phrase summarizing the factual subject matter required (e.g., \"Juno mission objectives and power\", \"Apollo 11 landing details\").\n",
        "\n",
        "    Respond ONLY with a JSON object containing these two keys.\"\"\"\n",
        "\n",
        "    # We request JSON mode for reliable parsing\n",
        "    analysis_result = call_llm(analysis_system_prompt, high_level_goal, json_mode=True)\n",
        "\n",
        "    try:\n",
        "        analysis = json.loads(analysis_result)\n",
        "        intent_query = analysis['intent_query']\n",
        "        topic_query = analysis['topic_query']\n",
        "    except (json.JSONDecodeError, KeyError):\n",
        "        print(f\"[Orchestrator] Error: Could not parse analysis JSON. Raw Analysis: {analysis_result}. Aborting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Orchestrator: Intent Query: '{intent_query}'\")\n",
        "    print(f\"Orchestrator: Topic Query: '{topic_query}'\")\n",
        "\n",
        "\n",
        "    # Step 1: Get the Context Blueprint (Procedural RAG)\n",
        "    mcp_to_librarian = create_mcp_message(\n",
        "        sender=\"Orchestrator\",\n",
        "        content={\"intent_query\": intent_query}\n",
        "    )\n",
        "    # display_mcp(mcp_to_librarian, \"Orchestrator -> Librarian\")\n",
        "    mcp_from_librarian = agent_context_librarian(mcp_to_librarian)\n",
        "    display_mcp(mcp_from_librarian, \"Librarian -> Orchestrator\")\n",
        "\n",
        "    context_blueprint = mcp_from_librarian['content'].get('blueprint')\n",
        "    if not context_blueprint: return\n",
        "\n",
        "    # Step 2: Get the Factual Knowledge (Factual RAG)\n",
        "    mcp_to_researcher = create_mcp_message(\n",
        "        sender=\"Orchestrator\",\n",
        "        content={\"topic_query\": topic_query}\n",
        "    )\n",
        "    # display_mcp(mcp_to_researcher, \"Orchestrator -> Researcher\")\n",
        "    mcp_from_researcher = agent_researcher(mcp_to_researcher)\n",
        "    display_mcp(mcp_from_researcher, \"Researcher -> Orchestrator\")\n",
        "\n",
        "    research_findings = mcp_from_researcher['content'].get('facts')\n",
        "    if not research_findings: return\n",
        "\n",
        "    # Step 3: Generate the Final Output\n",
        "    # Combine the outputs for the Writer Agent\n",
        "    writer_task = {\n",
        "        \"blueprint\": context_blueprint,\n",
        "        \"facts\": research_findings\n",
        "    }\n",
        "\n",
        "    mcp_to_writer = create_mcp_message(\n",
        "        sender=\"Orchestrator\",\n",
        "        content=writer_task\n",
        "    )\n",
        "    # display_mcp(mcp_to_writer, \"Orchestrator -> Writer\")\n",
        "    mcp_from_writer = agent_writer(mcp_to_writer)\n",
        "    display_mcp(mcp_from_writer, \"Writer -> Orchestrator\")\n",
        "\n",
        "    final_result = mcp_from_writer['content'].get('output')\n",
        "\n",
        "    print(\"\\n=== [Orchestrator] Task Complete ===\")\n",
        "    return final_result"
      ],
      "metadata": {
        "id": "kPEcJnBwLUQc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  6.Running examples\n"
      ],
      "metadata": {
        "id": "NcptoMmoLwKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example 1: Requesting a specific style (Suspense) for a topic (Apollo 11)\n",
        "print(\"********  1: SUSPENSEFUL NARRATIVE **********\")\n",
        "goal_1 = \"Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\"\n",
        "result_1 = orchestrator(goal_1)\n",
        "\n",
        "print(\"\\n******** FINAL OUTPUT 1 **********\\n\")\n",
        "print(result_1)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VclgEP1qLp5l",
        "outputId": "ecae11d2-c811-466d-ce04-f84294773391"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********  1: SUSPENSEFUL NARRATIVE **********\n",
            "=== [Orchestrator] Starting New Task ===\n",
            "Goal: Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\n",
            "\n",
            "[Orchestrator] Analyzing Goal...\n",
            "Orchestrator: Intent Query: 'short suspenseful children's story scene structure'\n",
            "Orchestrator: Topic Query: 'Apollo 11 moon landing dangers'\n",
            "\n",
            "[Librarian] Activated. Analyzing intent...\n",
            "[Librarian] Found blueprint 'blueprint_suspense_narrative' (Score: 0.57)\n",
            "\n",
            "--- Librarian -> Orchestrator (Sender: Librarian) ---\n",
            "Content Keys: ['blueprint']\n",
            "Metadata Keys: []\n",
            "--------------------------------------------------\n",
            "\n",
            "[Researcher] Activated. Investigating topic...\n",
            "[Researcher] Found 2 relevant chunks. Synthesizing...\n",
            "\n",
            "--- Researcher -> Orchestrator (Sender: Researcher) ---\n",
            "Content Keys: ['facts']\n",
            "Metadata Keys: []\n",
            "---------------------------------------------------\n",
            "\n",
            "[Writer] Activated. Applying blueprint to facts...\n",
            "\n",
            "--- Writer -> Orchestrator (Sender: Writer) ---\n",
            "Content Keys: ['output']\n",
            "Metadata Keys: []\n",
            "-----------------------------------------------\n",
            "\n",
            "=== [Orchestrator] Task Complete ===\n",
            "\n",
            "******** FINAL OUTPUT 1 **********\n",
            "\n",
            "The Moon hangs outside the window. Too bright. Too close.  \n",
            "\n",
            "The Agent floats in the cramped cabin, strapped in, gloves stiff, breath loud in the helmet. Every inhale is a hiss. Every exhale a rush.  \n",
            "\n",
            "“Program alarm.”  \n",
            "The voice cuts through the static. Sharp. Flat.  \n",
            "\n",
            "Numbers flicker. Lights blink. The Lunar Module shudders. The Source_of_Threat is invisible, buried in code and circuits and gravity. But it is here. In every warning tone. In every stray flicker on the guidance computer.  \n",
            "\n",
            "They are far from home. Too far.  \n",
            "They are dropping toward a world no one has ever touched.  \n",
            "\n",
            "The automatic system falters.  \n",
            "The computer overloads.  \n",
            "Alarms again.  \n",
            "\n",
            "The Agent’s fingers close around the control stick. Manual.  \n",
            "No more letting the machine choose.  \n",
            "\n",
            "Outside, the Moon’s surface crawls closer. Gray dust. Jagged shadows. Craters like open mouths.  \n",
            "No air. No sound. Only the groan of the engine and the mutter of voices in the headset, thin and distant from Earth.  \n",
            "\n",
            "Fuel ticks down.  \n",
            "Seconds bleed away.  \n",
            "The Source_of_Threat tightens: not a monster, not a ghost, but the cold fact of vacuum and velocity and mass. If the engine dies too high, they will fall. And fall. And never rise.  \n",
            "\n",
            "“Thirty seconds.”  \n",
            "The words crackle through. A countdown disguised as calm.  \n",
            "\n",
            "The Agent moves the thrusters, nudging the lander over a dark crater, away from boulders that loom like black teeth. Hands steady. Heart not.  \n",
            "\n",
            "Dust begins to stir below. A silent storm kicked up by the engine.  \n",
            "The surface is close enough to see the texture now. Grainy. Harsh.  \n",
            "No room for error. None.  \n",
            "\n",
            "“Contact light.”  \n",
            "A tiny blue glow. A soft indicator in a harsh world.  \n",
            "\n",
            "The engine cuts.  \n",
            "The Module settles. A faint thump runs up through metal, through the Agent’s boots, into bone. For a second, everything is too quiet. Too still.  \n",
            "\n",
            "Then the words:  \n",
            "“Tranquility Base here. The Eagle has landed.”  \n",
            "\n",
            "On Earth, millions lean toward their screens. Static snow. Ghostly images. No one can hear the danger now, but it lingers in the silence between words.  \n",
            "\n",
            "The hatch opens.  \n",
            "The ladder waits, narrow and steep. The Moon stretches below, a field of gray shadows under a black, empty sky.  \n",
            "\n",
            "The Agent steps down. One boot. Then the next.  \n",
            "Fine dust puffs up, slow and strange in the low gravity. It clings, then drifts away.  \n",
            "\n",
            "This place is utterly dead.  \n",
            "Yet the whole world listens.  \n",
            "\n",
            "“One small step…”  \n",
            "The voice trembles only slightly.  \n",
            "\n",
            "Beneath the words, the Source_of_Threat remains. The vacuum. The distance. The thin metal between life and nothing.  \n",
            "\n",
            "But for now, the Agent stands.  \n",
            "Alive.  \n",
            "On the Moon.\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example 2: Requesting a different style (Technical) for another topic (Juno)\n",
        "print(\"******** 2: TECHNICAL EXPLANATION **********\")\n",
        "goal_2 = \"Provide a clear technical explanation of the Juno mission's objectives and how it is powered.\"\n",
        "result_2 = orchestrator(goal_2)\n",
        "\n",
        "print(\"\\n******** FINAL OUTPUT 2 **********\\n\")\n",
        "print(result_2)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij6p9Ph5MCiC",
        "outputId": "8fefb04d-6581-4160-c127-7d09b86819f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******** 2: TECHNICAL EXPLANATION **********\n",
            "=== [Orchestrator] Starting New Task ===\n",
            "Goal: Provide a clear technical explanation of the Juno mission's objectives and how it is powered.\n",
            "\n",
            "[Orchestrator] Analyzing Goal...\n",
            "Orchestrator: Intent Query: 'clear technical explanation structure'\n",
            "Orchestrator: Topic Query: 'Juno mission objectives and power system'\n",
            "\n",
            "[Librarian] Activated. Analyzing intent...\n",
            "[Librarian] Found blueprint 'blueprint_technical_explanation' (Score: 0.49)\n",
            "\n",
            "--- Librarian -> Orchestrator (Sender: Librarian) ---\n",
            "Content Keys: ['blueprint']\n",
            "Metadata Keys: []\n",
            "--------------------------------------------------\n",
            "\n",
            "[Researcher] Activated. Investigating topic...\n",
            "[Researcher] Found 2 relevant chunks. Synthesizing...\n",
            "\n",
            "--- Researcher -> Orchestrator (Sender: Researcher) ---\n",
            "Content Keys: ['facts']\n",
            "Metadata Keys: []\n",
            "---------------------------------------------------\n",
            "\n",
            "[Writer] Activated. Applying blueprint to facts...\n",
            "\n",
            "--- Writer -> Orchestrator (Sender: Writer) ---\n",
            "Content Keys: ['output']\n",
            "Metadata Keys: []\n",
            "-----------------------------------------------\n",
            "\n",
            "=== [Orchestrator] Task Complete ===\n",
            "\n",
            "******** FINAL OUTPUT 2 **********\n",
            "\n",
            "**Definition**  \n",
            "Juno is a NASA space probe designed to study Jupiter. Launched on August 5, 2011, it entered a polar orbit around Jupiter on July 5, 2016. It is the second spacecraft to orbit Jupiter, following the Galileo orbiter, and is distinguished as the most distant solar-powered spacecraft from the Sun.\n",
            "\n",
            "**Function/Operation**  \n",
            "Juno operates in a highly elliptical polar orbit around Jupiter, enabling systematic coverage of the planet’s polar regions and global fields. The spacecraft is powered by large solar arrays rather than Radioisotope Thermoelectric Generators (RTGs), which is atypical for missions operating at Jupiter’s distance from the Sun. Its orbit and instrumentation are optimized to measure Jupiter’s composition, gravitational field, magnetic field, and polar magnetosphere.\n",
            "\n",
            "**Key Findings/Impact**  \n",
            "By characterizing Jupiter’s composition and mapping its gravitational and magnetic fields, Juno provides critical constraints on the planet’s internal structure and dynamics. The study of the polar magnetosphere yields insight into Jupiter’s powerful auroral processes and magnetospheric environment. Collectively, these measurements advance understanding of how Jupiter formed, offering key evidence for models of giant planet formation and, by extension, the early evolution of the solar system.\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Example 3: Requesting a casual style\n",
        "print(\"******** 3: CASUAL SUMMARY **********\")\n",
        "goal_3 = \"Give me a quick, casual summary of what Mars rovers do.\"\n",
        "result_3 = orchestrator(goal_3)\n",
        "\n",
        "print(\"\\n******** FINAL OUTPUT 3 **********\\n\")\n",
        "print(result_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkRChqNiMJOx",
        "outputId": "4abf374f-e2af-4a0e-9650-b34556fec975"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******** 3: CASUAL SUMMARY **********\n",
            "=== [Orchestrator] Starting New Task ===\n",
            "Goal: Give me a quick, casual summary of what Mars rovers do.\n",
            "\n",
            "[Orchestrator] Analyzing Goal...\n",
            "Orchestrator: Intent Query: 'quick casual explanatory summary'\n",
            "Orchestrator: Topic Query: 'purpose and activities of Mars rovers'\n",
            "\n",
            "[Librarian] Activated. Analyzing intent...\n",
            "[Librarian] Found blueprint 'blueprint_casual_summary' (Score: 0.64)\n",
            "\n",
            "--- Librarian -> Orchestrator (Sender: Librarian) ---\n",
            "Content Keys: ['blueprint']\n",
            "Metadata Keys: []\n",
            "--------------------------------------------------\n",
            "\n",
            "[Researcher] Activated. Investigating topic...\n",
            "[Researcher] Found 2 relevant chunks. Synthesizing...\n",
            "\n",
            "--- Researcher -> Orchestrator (Sender: Researcher) ---\n",
            "Content Keys: ['facts']\n",
            "Metadata Keys: []\n",
            "---------------------------------------------------\n",
            "\n",
            "[Writer] Activated. Applying blueprint to facts...\n",
            "\n",
            "--- Writer -> Orchestrator (Sender: Writer) ---\n",
            "Content Keys: ['output']\n",
            "Metadata Keys: []\n",
            "-----------------------------------------------\n",
            "\n",
            "=== [Orchestrator] Task Complete ===\n",
            "\n",
            "******** FINAL OUTPUT 3 **********\n",
            "\n",
            "Mars rovers are basically robot cars we send to drive around on Mars instead of just orbiting above it. They’re remote-controlled (with a lot of autonomy) and built to handle dust, rocks, and some pretty extreme conditions.\n",
            "\n",
            "NASA’s Jet Propulsion Lab (JPL) is the team behind the big-name rovers you’ve probably heard of:\n",
            "- Sojourner  \n",
            "- Spirit  \n",
            "- Opportunity  \n",
            "- Curiosity  \n",
            "- Perseverance  \n",
            "\n",
            "Their main mission these days? Figure out if Mars could have ever supported life. That means:\n",
            "- Checking if past environments were **habitable** (water, energy sources, right chemistry, etc.)  \n",
            "- Hunting for **organic carbon**, which is the kind of stuff that could be linked to past or present life.\n",
            "\n",
            "To do that, they:\n",
            "- Drive around to different regions and rock types  \n",
            "- Drill, sample, and analyze Martian rocks and soil  \n",
            "- Run experiments to see what the environment used to be like\n",
            "\n",
            "Perseverance has a cool extra: it brought the **Ingenuity** helicopter, which did the first powered flights on another planet. That let scientists scout the terrain from the air and test how flying works in Mars’ super thin atmosphere.\n"
          ]
        }
      ]
    }
  ]
}